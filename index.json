[{"authors":null,"categories":null,"content":"TRAILに関する最新情報をお届けします．\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matsuolab.github.io/roomba_hack/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/roomba_hack/authors/admin/","section":"authors","summary":"TRAILに関する最新情報をお届けします．","tags":null,"title":"TRAIL Admin","type":"authors"},{"authors":["jumpei-arima"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"451d184f9c1e53301faf72f3ade4d5c6","permalink":"https://matsuolab.github.io/roomba_hack/authors/jumpei-arima/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/roomba_hack/authors/jumpei-arima/","section":"authors","summary":"","tags":null,"title":"有馬 純平","type":"authors"},{"authors":["tatsuya-matsushima"],"categories":null,"content":"人間と共生できるような適応的なロボットの開発と，そのようなロボットを作ることにより生命性や知能を構成的に理解することに興味があります． 特に現在は，深層生成モデルを用いた環境のダイナミクスのモデリング（世界モデル）・モデルベース強化学習・メタ模倣学習に関して研究を行っています．\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"96eea6d6aacba886dc9c6ad4c3b6f83f","permalink":"https://matsuolab.github.io/roomba_hack/authors/tatsuya-matsushima/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/roomba_hack/authors/tatsuya-matsushima/","section":"authors","summary":"人間と共生できるような適応的なロボットの開発と，そのようなロ","tags":null,"title":"松嶋 達也","type":"authors"},{"authors":["yuya-ikeda"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4c9f99a2ef4096b81e6d0a08880581e8","permalink":"https://matsuolab.github.io/roomba_hack/authors/yuya-ikeda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/roomba_hack/authors/yuya-ikeda/","section":"authors","summary":"","tags":null,"title":"池田 悠也","type":"authors"},{"authors":null,"categories":null,"content":" 開発環境 ロボットシステムの開発環境に使われている要素の概要を理解する   ROSとは ロボット開発によく用いられるROSの概要を理解する   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"f1fb9b3e706d6a8e7af6ee8a67c187b1","permalink":"https://matsuolab.github.io/roomba_hack/course/chap1/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap1/","section":"course","summary":"ロボットシステムの基礎知識","tags":null,"title":"Chapter 1","type":"book"},{"authors":null,"categories":null,"content":" ROSのパッケージ・ワークスペース ROSのパッケージ管理について理解しよう   ロボットシステムにおけるセンシング・アクチュエーション・通信① センサの値を読み取りロボットを動かしてみよう   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b03bbdc309591542a3a0cf48966f3d87","permalink":"https://matsuolab.github.io/roomba_hack/course/chap2/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap2/","section":"course","summary":"センシング・アクチュエーション・通信①","tags":null,"title":"Chapter 2","type":"book"},{"authors":null,"categories":null,"content":" ロボットシステムにおけるセンシング・アクチュエーション・通信② 複数のセンサを組み合わせてよりかしこくロボットを動かしてみよう   ロボットシステムにおけるセンシング・アクチュエーション・通信③ 複数のセンサを組み合わせてよりかしこくロボットを動かしてみよう   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"b891a66c1b2d8231b078e52b380c46a1","permalink":"https://matsuolab.github.io/roomba_hack/course/chap3/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap3/","section":"course","summary":"センシング・アクチュエーション・通信②","tags":null,"title":"Chapter 3","type":"book"},{"authors":null,"categories":null,"content":" 自己位置推定   ナビゲーション   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"ef4ff386bad3919cafa8bb6a9000e1a5","permalink":"https://matsuolab.github.io/roomba_hack/course/chap4/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap4/","section":"course","summary":"自律移動","tags":null,"title":"Chapter 4","type":"book"},{"authors":null,"categories":null,"content":" 三次元画像処理   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c5b6edce25a3188d94c102a439d09587","permalink":"https://matsuolab.github.io/roomba_hack/course/chap5/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap5/","section":"course","summary":"3次元画像認識","tags":null,"title":"Chapter 5","type":"book"},{"authors":null,"categories":null,"content":" serviceとactionlib   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"8f714aaf4a960d30c5c5c987c0a4c5d1","permalink":"https://matsuolab.github.io/roomba_hack/course/chap6/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap6/","section":"course","summary":"複雑なロボットシステムの実装・分散処理","tags":null,"title":"Chapter 6","type":"book"},{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"2f5bc53e4f4c671993cb760f7d80fa51","permalink":"https://matsuolab.github.io/roomba_hack/course/chap7/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap7/","section":"course","summary":"最終プロジェクト準備","tags":null,"title":"Chapter 7","type":"book"},{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"2adf7951016803e68618153c45700bf0","permalink":"https://matsuolab.github.io/roomba_hack/course/chap8/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/roomba_hack/course/chap8/","section":"course","summary":"最終プロジェクト","tags":null,"title":"Chapter 8","type":"book"},{"authors":null,"categories":null,"content":"  -- Table of Contents  Program overview Courses in this program Meet your instructor    Python programming skills - Statistical concepts and how to apply them in practice - Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas -- Program overview 実ロボット(roomba)を利用した演習を通じて、ロボットシステムの仕組みから、センシング・認識・行動について研究開発に必要な最低限の知識や実装スキルを習得する。\nCourses in this program  Chapter 1 ロボットシステムの基礎知識\n  Chapter 2 センシング・アクチュエーション・通信①\n  Chapter 3 センシング・アクチュエーション・通信②\n  Chapter 4 自律移動\n  Chapter 5 3次元画像認識\n  Chapter 6 複雑なロボットシステムの実装・分散処理\n  Chapter 7 最終プロジェクト準備\n  Chapter 8 最終プロジェクト\n  Meet your instructor TRAIL Admin Are there prerequisites? There are no prerequisites for the first course.\n --  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"97ede4bb026d52eb5ee887238b8270ea","permalink":"https://matsuolab.github.io/roomba_hack/course/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/roomba_hack/course/","section":"course","summary":"rooombaを用いた実ロボットシステム入門","tags":null,"title":"📊 ロボットシステム入門","type":"book"},{"authors":null,"categories":null,"content":"複数のセンサを組み合わせてよりかしこくロボットを動かしてみよう\nLearn 前回の演習では，速度と時間の指令を使ってロボットを制御しました．\n周囲に障害物が何もない状況や，ロボットの滑りがない環境では，速度と時間のコマンドを使って思った通りにロボットを動かすことができるかもしれませんが，実環境では，ロボットの周囲には障害物が存在しますし，移動距離で制御する方が直感的です．\n前回の演習のようにロボットに速度と時間を一回与えて，その通りに動かすようなフィードフォワード制御ではなく，今回は，ロボットが逐次的にセンサの情報を反映して振る舞いを変えるフィードバック制御を行なってみましょう．\nオドメトリのセンサ情報を使ってロボットを動かしてみよう まずは，ロボットのタイヤの回転量から計算される移動距離である（ホイール）オドメトリ（odometry）を使った制御をしてみましょう．\nオドメトリのメッセージ（/odom）の中身を見てみよう roombaのオドメトリの情報は，/odomトピックにpublishされています．\nrostopic echo /odomをしてみるとメッセージとしてどんな情報が流れているかわかります． rostopic echo -n 1 /odom root@dynamics:~/roomba_hack# rostopic echo -n 1 /odom header: seq: 2115 stamp: secs: 1649692132 nsecs: 791056254 frame_id: \u0026quot;odom\u0026quot; child_frame_id: \u0026quot;base_footprint\u0026quot; pose: pose: position: x: -0.014664691872894764 y: -0.0010878229513764381 z: 0.0 orientation: x: 0.0 y: 0.0 z: 0.0056752621080531414 w: 0.9999838955703261 covariance: [0.08313143998384476, 0.00019857974257320166, 0.0, 0.0, 0.0, 0.004368376452475786, 0.00019857988809235394, 0.015032557770609856, 0.0, 0.0, 0.0, -0.26573312282562256, 0.0, 0.0, 1e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-05, 0.0, 0.0043683769181370735, -0.26573312282562256, 0.0, 0.0, 0.0, 6.021446704864502] twist: twist: linear: x: 0.0 y: 0.0 z: 0.0 angular: x: 0.0 y: 0.0 z: 0.0 covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ---  \nrostopic type /odomをしてみると，メッセージとして，nav_msgs/Odometry型が使われていることがわかります． rostopic type /odom root@dynamics:~/roomba_hack# rostopic type /odom nav_msgs/Odometry  \nnav_msgs/Odometry型のドキュメントを確認してみると，このメッセージはposeとtwistで構成されていることがわかります．\nposeは．（child_frameから見た）ロボットの推定姿勢（位置と回転角）を表していて，covarianceにはその不確かさを表す共分散が記録されています．\n一方，twistは，（child_frameから見た）ロボットの速度を表していて，poseと同様にcovarianceにはその不確かさを表す共分散が記録されています．\nなお，メッセージ型の定義は，rosmsg info nav_msgs/Odometryすることでもコマンドから確認できます． rosmsg info nav_msgs/Odometry root@dynamics:~/roomba_hack# rosmsg info nav_msgs/Odometry std_msgs/Header header uint32 seq time stamp string frame_id string child_frame_id geometry_msgs/PoseWithCovariance pose geometry_msgs/Pose pose geometry_msgs/Point position float64 x float64 y float64 z geometry_msgs/Quaternion orientation float64 x float64 y float64 z float64 w float64[36] covariance geometry_msgs/TwistWithCovariance twist geometry_msgs/Twist twist geometry_msgs/Vector3 linear float64 x float64 y float64 z geometry_msgs/Vector3 angular float64 x float64 y float64 z float64[36] covariance  \nクォータニオン(quaternion) さて，/odomのトピックでは，ロボットの回転角はクォータニオン（quaternion）で記述されています．\nクォータニオンは，日本語では四元数と呼ばれ，3次元空間上での回転角を表現する方法の一つで，4つの要素を持つベクトルで表現されます．\nクォータニオンによる3次元回転の表現は，角度を連続的にかつ簡潔に表現できるためROSではよく用いられます（その他には，オイラー角による表現や回転行列による表現があります）．\nそれぞれの回転角に関する表現のメリット・デメリットを調べてみましょう（「ジンバルロック」などのキーワードで調べるとよりよく理解できると思います）．\nクォータニオンからオイラー角へは，tfパッケージのtf.transformations.euler_from_quaternionを使うことで変換できます（ドキュメント）．\nサブスクライバ（subscriber)の仕組みを知ろう それでは，オドメトリ/odomの情報を使った制御の実装の例としてnavigation_tutorialパッケージの中のsimple_control2.pyを見てみましょう（github）．\n前回までに強調されてきた通り，ROSは非同期分散のシステムを簡単に作ることができるのが特徴です． そのため，ロボットから非同期に送られてくる/odomの情報をうまく扱うことが重要です．\n実装例にあるように，Pythonによるノードの実装では，クラスとして定義するのがわかりやすい方法でしょう．\n実装例では，SimpleControlllerクラスとして，simple_controllerというノードを定義しています． 以下のように，ノードを初期化する際に，コマンドを/cmd_velトピックに送信するパブリッシャ（publisher)と，/odomを受信するサブスクライバ(subscriber)を作成しています．\nclass SimpleController: def __init__(self): rospy.init_node('simple_controller', anonymous=True) # Publisher self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10) # Subscriber odom_sub = rospy.Subscriber('/odom', Odometry, self.callback_odom) self.x = None self.y = None self.yaw = None while self.x is None: rospy.sleep(0.1)  パブリッシャの使い方は前回のsimple_control.pyの実装を確認してください．\nパブリッシャと同様に，サブスクライバはrospyのSubscriberを用いて作成できます． サブスクライバの特徴として，メッセージを受信した時の処理であるコールバック（callback）を定義できます．\nこの実装例では，self.callback_odomとして定義されており，インスタンスの属性（self.x, self.y, self.yaw）を，受信したメッセージで変更するようなプログラムになっています．\ndef callback_odom(self, data): self.x = data.pose.pose.position.x self.y = data.pose.pose.position.y self.yaw = self.get_yaw_from_quaternion(data.pose.pose.orientation)  つまり，self.xには/odomから受信した位置のx座標，self.yには位置のy座標，self.yawには，回転角のyawを格納しています．\nクォータニオンとして受信した姿勢の回転角のyaw成分を取り出すためのself.get_yaw_from_quaternionは以下のようになっています（オイラー角はroll, pitch, yawの順で返ってくるのでe[2]でyawを取得しています）．\ndef get_yaw_from_quaternion(self, quaternion): e = tf.transformations.euler_from_quaternion( (quaternion.x, quaternion.y, quaternion.z, quaternion.w)) return e[2]  これらのセンサの値を使うことで，以下のように，指定した距離ロボットが移動するまで直進させ続けたり，指定した角度までロボットが回転するまで回転させ続けることができるようになります．\n直進\ndef go_straight(self, dis, velocity=0.3): vel = Twist() x0 = self.x y0 = self.y while(np.sqrt((self.x-x0)**2+(self.y-y0)**2)\u0026lt;dis): vel.linear.x = velocity vel.angular.z = 0.0 self.cmd_vel_pub.publish(vel) rospy.sleep(0.1) self.stop()  右回転\ndef turn_right(self, yaw, yawrate=-0.5): vel = Twist() yaw0 = self.yaw while(abs(self.yaw-yaw0)\u0026lt;np.deg2rad(yaw)): vel.linear.x = 0.0 vel.angular.z = yawrate self.cmd_vel_pub.publish(vel) rospy.sleep(0.1) self.stop()  左回転\ndef turn_left(self, yaw, yawrate=0.5): vel = Twist() yaw0 = self.yaw while(abs(self.yaw-yaw0)\u0026lt;np.deg2rad(yaw)): vel.linear.x = 0.0 vel.angular.z = yawrate self.cmd_vel_pub.publish(vel) rospy.sleep(0.1) self.stop()  それでは，オドメトリを使って実際にロボットを制御してみましょう．\n演習 【jetson・開発マシン】それぞれdockerコンテナを起動 ． jetsonでdockerコンテナを起動\n(開発PC):~$ ssh roomba_dev1 (jetson):~$ cd ~/group_a/roomba_hack (jetson):~/group_a/roomba_hack ./RUN-DOCKER-CONTAINER.sh (jetson)(docker):~/roomba_hack#  開発PCでdockerコンテナを起動\n(開発PC):~$ cd ~/group_a/roomba_hack (開発PC):~/group_a/roomba_hack ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x (開発PC)(docker):~/roomba_hack#   【jetson】ROSマスタ、各種ノードを起動 (jetson)(docker):~/roomba_hack# roslaunch roomba_bringup bringup.launch   ROSメッセージの可視化 【開発PC】topicの確認 /odomの型を確認\n(開発PC)(docker):~/roomba_hack# rostopic type /odom  /odomの中身を確認\n(開発PC)(docker):~/roomba_hack# rostopic echo /odom   オドメトリを使ったフィードバック制御 simple_control2.pyを実行してみよう．\n開発PCでteleopのコードを実行しましょう\n(開発PC)(docker):~/roomba_hack# roslaunch roomba_teleop teleop.launch  このプログラムを動かすときには，コントローラのYボタンを押してからBボタンを押してautoモードにしておきましょう．\n1メートルほど前に進んだあと，左に90度程度旋回し，右に90度程度旋回したら成功です．\n(開発PC)(docker):~/roomba_hack# rosrun navigation_tutorial simple_control2.py  try it! simple_control2.pyの中身を読んでコードを変更してみよう\n","date":1649116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649116800,"objectID":"2ee7ed9f0716848dbd6ffe877377e500","permalink":"https://matsuolab.github.io/roomba_hack/course/chap3/sensing2/","publishdate":"2022-04-05T00:00:00Z","relpermalink":"/roomba_hack/course/chap3/sensing2/","section":"course","summary":"複数のセンサを組み合わせてよりかしこくロボットを動かしてみよう\n","tags":null,"title":"ロボットシステムにおけるセンシング・アクチュエーション・通信②","type":"book"},{"authors":null,"categories":null,"content":"Learn ここまでトピックを使った通信を使ってロボットシステムを構築してきました． トピック通信は，メッセージを出版(publish，配信とも訳される)・購読（subscribe）することで通信する，相手を仮定しない非同期な通信方法でした．\nしかし，もっと複雑なシステムを組む場合には，「相手の処理の結果を呼び出し側で受け取って知りたい」など様々な場合が考えられます．\nこのようなより複雑な通信を実現するための通信方式として，ROSにはサービス（service）とアクション（actionlib）が用意されています．\nservice これまで利用してきたトピック通信は，通信の相手を仮定しない（相手がいようといまいと関係ない）ため，ロボットシステムに特有な非同期通信・処理を実現するために簡単な方法でした．\n一方で，他のノードに対して「特定の処理の依頼をして，その結果を待ちたい」場合など，同期的・双方向な通信が必要になることがあります． 例えば，あるノードの設定を変更をして，それが成功したかどうかを知りたい場合などに使えます． サービスを使った通信は，「クライアント・サーバ」型の通信（クライアントサーバモデル, client-server model）となり，クライアントがサーバにリクエストを送ると，サーバがレスポンスを返すような仕組みになっています．\npythonではrospy.serviceを使ってサーバを，rospy.service_proxyを使ってクライアントを簡単に実装できます（参考URL）．\nまた，コマンドラインからは\nrosservice call [service] [args]  として，簡単にクライアントを作成できますし，システム上に存在するサービスの一覧は\nrosservice list  とすることで表示できます．あるサービスのメッセージがどのように定義されているかは，\nrosservice type [service]  で調べられます．\nactionlib ここまで，トピック通信を使うことで相手を仮定しない非同期通信を，サービスを使った通信を行うことで相手のレスポンスを待つ同期的な通信を実現できることを見てきました．\nサービスによる通信では，クライアントはサーバからのレスポンスを待つため，サーバで長い時間がかかるような処理を行う（計算量が大きい，または，移動に時間がかかるなど）場合には，クライアントの処理が長い間停止してしまうという問題があります．\nそのため，処理の呼び出し側のプログラムをブロックせずに，かつ，処理の結果（や途中経過）を知れるような非同期通信が欲しくなります． この要求を満たすのが，ROSのアクション(actionlib)です．\nactionlibは，実はトピック通信の組み合わせとして構成されており，goal（命令），result（処理の結果），feedback（途中経過），status（サーバの状態），cancel（命令の取り消し）の5つのトピックからなります． このあたりの仕様は，qiitaのROS講座が詳しいので参照してください．\npythonでは，actionlibのサーバやクライアントも，\nimport actionlib  したのちに，他の通信方式と同様にactionlib.SimpleActionServerとして，簡単に作成できます（ドキュメント）．\n今回の演習では，簡単のためaction serverの作成は行いません． 変わりに，移動のためのactionとして，move_baseパッケージの中で定義されているmove_baseというactionを使うことにしましょう．\n実はこのパッケージは\nroslaunch navigation_tutorial navigation.launch  してmove_baseノードを起動した際に既に利用されていました（これまでは，そのパッケージの中でサブスクライバとして定義されたmove_base_simple/goalというトピックにpublishすることで移動をしていました）．\nmove_baseのパッケージの詳細はドキュメントを見て確認してみてください．\n同様に，action clientもactionlib.SimpleActionClientを利用することで簡単に作成できます．\n例えば，move_baseのaction clientの実装する際には，\nimport actionlib import tf from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal from geometry_msgs.msg import Quaternion action_client = actionlib.SimpleActionClient('move_base', MoveBaseAction) action_client.wait_for_server() # action serverの準備ができるまで待つ goal = MoveBaseGoal() # goalのメッセージの定義 goal.target_pose.header.frame_id = 'map' # マップ座標系でのゴールとして設定 goal.target_pose.header.stamp = rospy.Time.now() # 現在時刻 # ゴールの姿勢を指定 goal.target_pose.pose.position.x = X goal.target_pose.pose.position.y = Y q = uaternion_from_euler(0, 0, YAW) # 回転はquartanionで記述するので変換 goal.target_pose.pose.orientation = Quaternion(q[0], q[1], q[2], q[3]) action_client.send_goal(goal) # ゴールを命令  のようにクライアントのsend_goalメソッドでゴールを指定できます．\nその後，\naction_client.wait_for_result(rospy.Duration(30))  とすると，結果が返ってくるまで（この場合30秒間），クライアントの処理をブロックできますし，\nresult = action_client.wait_for_result(rospy.Duration(30))  とすることで，result変数に処理の結果が格納され，確認できます．\n演習 【jetson・開発マシン】起動準備 cd roomba_hack git fetch git checkout feature/integrate (jetson) ./RUN-DOCKER-CONTAINER.sh (開発マシン) ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x   【開発マシン】scriptベースのnavigationを実行してみる (開発マシン)(docker) roslaunch navigation_turtorial navigation.launch (開発マシン)(docker) rosrun navigation_turtorial topic_goal.py (開発マシン)(docker) rosrun navigation_turtorial action_goal.py   【開発マシン】RealSenseで検出した障害物をコストマップに追加してみよう (開発マシン)(docker) roslaunch three-dimensions_tutorial detection_pc.launch   （総合課題）障害物を避けながらnavigationする Lidarに映らない物体も画像ベースで検出しコストマップに追加することでナビゲーション時にぶつからないようにしましょう。\nヒント\n 物体検出結果に基づいて物体部分以外をマスクしたデプス画像をpublishする depth2pc.launchでそれをsubscribeし、point(cloud)に変換する 変換されたpointからmap座標系での位置を取得する costmapに反映する move_baseアクションを使ってナビゲーションを実装しよう．  するとactionがタイムアウトした場合や，KeyboardInterruptされた場合にcancel_goalメソッドを使うことでactionをキャンセルできるように拡張できるはずです．    さらに，PyTorchを使用した自作の分類器やネット上の分類器をシステムに組み込んで（例えばセグメンテーションモデルなど），よりよく動作するように改良してみましょう．\n","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642809600,"objectID":"ba96fee21f24c18e95f236953fe70e74","permalink":"https://matsuolab.github.io/roomba_hack/course/chap6/service-actionlib/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/roomba_hack/course/chap6/service-actionlib/","section":"course","summary":"","tags":null,"title":"serviceとactionlib","type":"book"},{"authors":null,"categories":null,"content":"Learn 今回はRealSenseD435というRGBDカメラを用いて三次元画像処理を行っていきましょう。\nRGBDカメラについて RGBDカメラとは、カラーの他にデプス(深度)を取得できるカメラのことです。 複雑な動作を行うロボットを動かす際には三次元空間の把握が重要となり、RGBDカメラはよく用いられます。 比較的安価でよく利用されるRGBDカメラとして、Intel社製のRealSenseやMicrosoft社製のXtionなどがあります。\nRealSense 今回はRGBDカメラとしてRealSenseD435を使用します。\nROSで用いる際には標準のラッパー(https://github.com/IntelRealSense/realsense-ros)を使用します。\nroslaunch realsense2_camera rs_camera.launchを行うとデフォルトのトピックとして RGB画像の/camera/color/image_raw、 デプス画像の/camera/depth/image_raw が利用できます。これらのトピックはいずれもsensor_msgs/Image型です。\nRealSenseは物理的にRGB画像モジュールとデプス画像モジュールが離れているため、これら2つのトピックはいずれも画像データではあるものの、ピクセルの位置関係が対応しておらずそのままだとうまく画像処理に用いることができませんが、起動時にalign:=trueを指定すると、デプス画像をRGB画像のピクセルに対応するように変換された/camera/aligned_depth_to_color/image_rawトピックを使用できるようになります。\n物体検出 まずはRGB画像/camera/color/image_rawのみを用いて三次元ではない画像検出を行っていきましょう。\n以下は/camera/color/image_rawをSubscribeし、物体検出アルゴリズムであるYOLOv3に入力し、その結果をbounding boxとして描画し、/detection_resultとしてPublishするスクリプトです。\n#!/usr/bin/env python3 import rospy from sensor_msgs.msg import Image from cv_bridge import CvBridge from pytorchyolo import detect, models import matplotlib.pyplot as plt import matplotlib.patches as patches import cv2 import copy class ObjectDetection: def __init__(self): rospy.init_node('object_detection', anonymous=True) # Publisher self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10) # Subscriber rgb_sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback_rgb) self.bridge = CvBridge() self.rgb_image = None def callback_rgb(self, data): cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8') cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB) self.rgb_image = cv_array def process(self): path = \u0026quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/\u0026quot; # load category with open(path+\u0026quot;data/coco.names\u0026quot;) as f: category = f.read().splitlines() # prepare model model = models.load_model(path+\u0026quot;config/yolov3.cfg\u0026quot;, path+\u0026quot;weights/yolov3.weights\u0026quot;) while not rospy.is_shutdown(): if self.rgb_image is None: continue # inference tmp_image = copy.copy(self.rgb_image) boxes = detect.detect_image(model, tmp_image) # [[x1, y1, x2, y2, confidence, class]] # plot bouding box for box in boxes: x1, y1, x2, y2 = map(int, box[:4]) cls_pred = int(box[5]) tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3) tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2) # publish image tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR) detection_result = self.bridge.cv2_to_imgmsg(tmp_image, \u0026quot;bgr8\u0026quot;) self.detection_result_pub.publish(detection_result) if __name__ == '__main__': od = ObjectDetection() try: od.process() except rospy.ROSInitException: pass  コールバック関数でsensor_msgs/Image型をnp.ndarray型に変換するために\ncv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8') cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)  というsensor_msgs/Image型特有の処理を行ってますが、Subscriberを作成しコールバック関数でデータを受け取るという基本的な処理の流れはscanなどの他のセンサと同じです。\nここで注意してほしいのはYOLOの推論部分をコールバック関数内で行っていないことです。 一見、新しいデータが入ってくるときのみに推論を回すことは合理的に見えますが、センサの入力に対してコールバック関数内の処理が重いとセンサの入力がどんどん遅れていってしまいます。 コールバック関数内ではセンサデータの最低限の処理の記述にとどめ、重い処理は分けて書くことを意識しましょう。\nまた、ここでは既存の物体検出モジュールを使用しましたが、PyTorchなどで作成した自作のモデルも同様の枠組みで利用することができます。\n続いて、RGB画像に整列されたデプス画像データを統合して物体を検出し、物体までの距離を測定してみましょう。\nRGB画像/camera/color/image_rawと整列されたデプス画像/camera/aligned_depth_to_color/image_rawはそれぞれ独立したトピックであるため、同期を取る必要があります。\n画像の同期にはmessage_filters(http://wiki.ros.org/message_filters)がよく使われます。\nmessage_filters.ApproximateTimeSynchronizerを使い以下のようにSubscriberを作成します。\nrgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image) depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image) message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(callback_rgbd) def callback_rgbd(data1, data2): bridge = CvBridge() cv_array = bridge.imgmsg_to_cv2(data1, 'bgr8') cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB) self.rgb_image = cv_array cv_array = bridge.imgmsg_to_cv2(data2, 'passthrough') self.depth_image = cv_array  この例では、1.0秒の許容で'/camera/color/image_raw\u0026rsquo;と'/camera/aligned_depth_to_color/image_raw\u0026rsquo;のトピックの同期を取ることができれば、コールバック関数callback_rgbdが呼ばれセンサデータが受けとられます。\nそれでは、物体を検出し、物体までの距離を測定するスクリプトを見てみましょう。\n#!/usr/bin/env python3 import rospy import message_filters from sensor_msgs.msg import Image from cv_bridge import CvBridge from pytorchyolo import detect, models import matplotlib.pyplot as plt import matplotlib.patches as patches import cv2 import copy class DetectionDistance: def __init__(self): rospy.init_node('detection_distance', anonymous=True) # Publisher self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10) # Subscriber rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image) depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image) message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(self.callback_rgbd) self.bridge = CvBridge() self.rgb_image, self.depth_image = None, None def callback_rgbd(self, data1, data2): cv_array = self.bridge.imgmsg_to_cv2(data1, 'bgr8') cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB) self.rgb_image = cv_array cv_array = self.bridge.imgmsg_to_cv2(data2, 'passthrough') self.depth_image = cv_array def process(self): path = \u0026quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/\u0026quot; # load category with open(path+\u0026quot;data/coco.names\u0026quot;) as f: category = f.read().splitlines() # prepare model model = models.load_model(path+\u0026quot;config/yolov3.cfg\u0026quot;, path+\u0026quot;weights/yolov3.weights\u0026quot;) while not rospy.is_shutdown(): if self.rgb_image is None: continue # inference tmp_image = copy.copy(self.rgb_image) boxes = detect.detect_image(model, tmp_image) # [[x1, y1, x2, y2, confidence, class]] # plot bouding box for box in boxes: x1, y1, x2, y2 = map(int, box[:4]) cls_pred = int(box[5]) tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3) tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2) cx, cy = (x1+x2)//2, (y1+y2)//2 print(category[cls_pred], self.depth_image[cy][cx]/1000, \u0026quot;m\u0026quot;) # publish image tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR) detection_result = self.bridge.cv2_to_imgmsg(tmp_image, \u0026quot;bgr8\u0026quot;) self.detection_result_pub.publish(detection_result) if __name__ == '__main__': dd = DetectionDistance() try: dd.process() except rospy.ROSInitException:  基本的には物体検出のスクリプトと同じですが、\ncx, cy = (x1+x2)//2, (y1+y2)//2 print(category[cls_pred], self.depth_image[cy][cx]/1000, \u0026quot;m\u0026quot;)  でbounding boxの中心座標を変換し、対応する距離をメートル単位で表示しています。\n整列されたデプス画像を用いているため、RGB画像に基づき算出した座標をそのまま指定できます。\n点群の作成 上の例ではRGB画像とデプス画像を用いた三次元画像処理を行うことができました。\nしかし、ロボットの自立移動などより複雑な動作をさせることを考えたとき、深度データを三次元空間にマッピングできたほうが位置関係を統一的に扱うことができ便利なこともあります。\nそれでデプス画像から点群と呼ばれるデータを作成することを考えます。\n点群とは三次元座標値(X,Y,Z)で構成された点の集まりのことです。各点の情報として、三次元座標値に加え色の情報(R,G,B)が加わることもあります。 デプス画像はカメラの内部パラメータを用いることによって点群データに変換することができます。(https://medium.com/yodayoda/from-depth-map-to-point-cloud-7473721d3f)\n今回はdepth_image_procと呼ばれる、デプス画像を点群データに変換するROSの外部パッケージ(http://wiki.ros.org/depth_image_proc) を使用して点群の変換を行います。\n外部パッケージは~/catkin_ws/src等のワークスペースに配置し、ビルドしパスを通すことで簡単に使用できます。\ndepth_image_procのwikiを参考に以下のようなlaunchファイルを作成しました。\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;launch\u0026gt; \u0026lt;node pkg=\u0026quot;nodelet\u0026quot; type=\u0026quot;nodelet\u0026quot; name=\u0026quot;nodelet_manager\u0026quot; args=\u0026quot;manager\u0026quot; /\u0026gt; \u0026lt;node pkg=\u0026quot;nodelet\u0026quot; type=\u0026quot;nodelet\u0026quot; name=\u0026quot;nodelet1\u0026quot; args=\u0026quot;load depth_image_proc/point_cloud_xyz nodelet_manager\u0026quot;\u0026gt; \u0026lt;remap from=\u0026quot;camera_info\u0026quot; to=\u0026quot;/camera/color/camera_info\u0026quot;/\u0026gt; \u0026lt;remap from=\u0026quot;image_rect\u0026quot; to=\u0026quot;/camera/aligned_depth_to_color/image_raw\u0026quot;/\u0026gt; \u0026lt;remap from=\u0026quot;points\u0026quot; to=\u0026quot;/camera/depth/points\u0026quot;/\u0026gt; \u0026lt;/node\u0026gt; \u0026lt;/launch\u0026gt;  このlaunchファイルを実行すると/camera/color/camera_infoと/camera/aligned_depth_to_color/image_rawをSubscribeし、/camera/depth/pointsをPublishします。\n/camera/color/camera_infoはsensor_msgs/CameraInfo型のトピックであり、カメラパラメータやフレームid、タイムスタンプなどを保持しており、点群の変換に利用されます。 /camera/aligned_depth_to_color/image_rawはRGB画像に整列されたデプス画像であるため、/camera/depth/camera_infoではなく/camera/color/camera_infoを指定することに注意してください。\nroslaunch three-dimensions_tutorial depth2pc.launchを行い/camera/depth/pointsトピックをrvizで可視化をすると三次元空間に点群データが表示されているのが確認できます。\n演習 (開発PC, jetson)起動準備 (jetson)$ ./RUN-DOCKER-CONTAINER.sh (jetson)(docker)# roslaunch roomba_bringup bringup.launch (開発PC)$ ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x   (開発PC)RealSenseのトピックの可視化 (開発PC)(docker) rviz  /camera/color/image_rawと/camera/depth/image_rawと/camera/aligned_depth_to_color/image_rawを可視化して違いを確認してみよう。\n (開発PC)物体検出を行う (開発PC)(docker) cd catkin_ws; catkin_make; source devel/setup.bash (開発PC)(docker) roscd three-dimensions_tutorial; cd yolov3/weights; ./download_weights.sh (開発PC)(docker) rosrun three-dimensions_tutorial object_detection.py rvizで`/detection_result`を表示し結果を確認してみよう。 (開発PC)(docker) rosrun three-dimensions_tutorial detection_distance.py   (開発PC)外部パッケージを使用 (開発PC)(docker) cd ~/external_catkin_ws/src (開発PC)(docker) git clone https://github.com/ros-perception/image_pipeline (開発PC)(docker) cd ../; catkin build; source devel/setup.bash (開発PC)(docker) cd ~/roomba_hack/catkin_ws; source devel/setup.bash (開発PC)(docker) roslaunch three-dimensions_tutorial depth2pc.launch (開発PC)(docker) roslaunch navigation_tutorial navigation.launch  rvizで/camera/depth/pointsトピックを追加して確認してみよう。\n 余裕がある人向け 物体を検出し、特定の物体の手前まで移動するスクリプトを作ってみましょう。\nヒント\n 物体検出結果に基づいて物体部分以外をマスクしたデプス画像をpublishする depth2pc.launchでそれをsubscribeし、point(cloud)に変換する 変換されたpointからmap座標系での位置を取得する navigation_tutorial/scripts/set_goal.py (map座標系で指定した位置・姿勢までナビゲーションするスクリプト)などを参考に、その位置へとナビゲーションする  PyTorchを使用した自作の分類器やネット上の分類器をシステムに組み込んでみましょう。\nLidarに映らない物体も画像ベースで検出しコストマップに追加することでナビゲーション時にぶつからないようにしましょう。\n","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642809600,"objectID":"6efadb7100ad083ad0bc43c5fd3c3b1a","permalink":"https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/roomba_hack/course/chap5/three-dimensions/","section":"course","summary":"","tags":null,"title":"三次元画像処理","type":"book"},{"authors":null,"categories":null,"content":"Learn 前回の演習では、オドメトリを用いてロボットを制御しました。\n1m進むや、90度右回転などある程度正確に動いたかと思います。 しかし、これが数10m前進や、数分間動き続けた時にロボット自身は自分がスタートの時からどのくらい動いたかわかるでしょうか。\nルンバが用いているホイールオドメトリは、ホイールの回転量を積算することで算出しています。ホイールが滑った場合だけでなく、センサの僅かの誤差の積み重ねで徐々にずれていってしまいます。\nそこで今回は、オドメトリ情報だけでなく、地図とLiDARスキャン情報も同時に使いながら、ロボット自身の尤もらしい位置を推定していきましょう。\nROSにおける座標系の扱い方(TF) まずは、ROSにおける座標系の扱い方についてみていきましょう。 ロボットシステムは、いろいろな座標系を使って位置姿勢を表現することが多いです。\n ロボットの座標系 センサの座標系 ロボットの関節の座標系 部屋の座標系 物体の座標系 ・・・・  このような座標系同士を繋げてロボットシステム上での座標系の管理をしてくれるROSのモジュールとしてtfがあります。 tfは、各座標系をツリー上で繋げます。従って、親の座標系が複数あることは許されません。\n今回自己位置推定するにあたり用いる座標系の関係は以下のようになります。\nrosrun rqt_tf_tree rqt_tf_treeとしてみると、tfのツリー形状を可視化することができます。\n  tfツリーをrqtで可視化  ここで、odom座標系は、オドメトリの算出を始めた位置(起動した位置)を原点とした座標系で、ホイールオドメトリの値から、ロボットの基準となるbase_footprint座標系を繋げています。 base_footprint座標系の下には、ルンバロボットの構成要素である、センサ類やホイールなどの座標系が子として繋がっています。\n一番親にいるmap座標系は、地図の原点を基準とした座標系ですが、この座標系におけるロボットの座標系(base_footprint)を繋げることが、自己位置推定の目的になります。 しかし、base_footprintの親には既にodomがいるため、map座標系とodom座標系を繋げることで、全体をひとつのツリーとして管理することができます。\n自己位置推定 自己位置推定は、地図が事前に与えられていて、そこのどこにロボットがいるかを逐次的に外界センサ(LiDAR)と内界センサ(Odometry)を用いて推定していく手法になります。\nヒストグラムフィルタやカルマンフィルタ、パーティクルフィルタなどいくつかの手法が存在し、 それぞれメリットデメリットがありますが、ここでは代表的なパーティクルフィルタを用いた手法を紹介します。\n自己位置推定では、観測モデルと状態遷移モデルを交互に繰り返すことによって、ロボット自身がどこにいるかの確率分布を更新していくことで自己位置推定をしていきます。\nパーティクルフィルタでは、この確率分布を大量の粒子を用いて表現する手法になっていて、各粒子が位置とそこにロボットがいるであろう確率(尤度)を持っています。\nロボットが動くごと(オドメトリが更新されるごと)に、状態遷移モデルを用いて各粒子の位置情報を更新します。 この時、一般的に分布は広がります。(人間が目を閉じて歩いたらどこにいるか分かりづらくなるのと同じ)\n外界の情報がわかるごと(スキャン情報が更新されるごと)に、観測モデルを用いて各粒子の尤度を更新します。 尤度は、各粒子の位置から観測できるであろうスキャン情報と、実際のロボットで取得したスキャン情報との差から算出します。\n  Monte Carlo Localization(Particle Filter) Dieter Fox et al. 1999, using sonar. http://www.doc.ic.ac.uk/~ajd/Robotics/RoboticsResources/montecarlolocalization.gif  launchファイルとrosparam 自己位置推定では、初期位置がどこか、レーザーのスペックや、パーティクルの数など数十個のパラメータを保持します。\nこれらをプログラム内部で記述するのではなく、launchファイル内で指定することが可能です。 rosでは、rosparamという形でパラメータを管理することが可能です。\n以下に、今回用いるamcl.launch を示します。 launchファイルはxml形式で記述され、paramを指定すること以外にも、 launchファイル実行時に引数で指定可能なargや、トピック名などのリマップをすることも可能です。\nlaunchの詳しい書き方は、rosのドキュメントを参照してください。\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;launch\u0026gt; \u0026lt;arg name=\u0026quot;use_map_topic\u0026quot; default=\u0026quot;true\u0026quot;/\u0026gt; \u0026lt;arg name=\u0026quot;odom_topic\u0026quot; default=\u0026quot;/odom\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;scan_topic\u0026quot; default=\u0026quot;/scan\u0026quot; /\u0026gt; \u0026lt;node pkg=\u0026quot;amcl\u0026quot; type=\u0026quot;amcl\u0026quot; name=\u0026quot;amcl\u0026quot; output=\u0026quot;screen\u0026quot;\u0026gt; \u0026lt;remap from=\u0026quot;scan\u0026quot; to=\u0026quot;$(arg scan_topic)\u0026quot;/\u0026gt; \u0026lt;remap from=\u0026quot;odom\u0026quot; to=\u0026quot;$(arg odom_topic)\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;use_map_topic\u0026quot; value=\u0026quot;$(arg use_map_topic)\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;initial_pose_x\u0026quot; value=\u0026quot;0.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;initial_pose_y\u0026quot; value=\u0026quot;0.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;initial_pose_a\u0026quot; value=\u0026quot;0.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;initial_cov_xx\u0026quot; value=\u0026quot;0.1*0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;initial_cov_yy\u0026quot; value=\u0026quot;0.1*0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;initial_cov_aa\u0026quot; value=\u0026quot;0.3*3.14\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;gui_publish_rate\u0026quot; value=\u0026quot;10.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_max_beams\u0026quot; value=\u0026quot;2.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_min_range\u0026quot; value=\u0026quot;0.15\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_max_range\u0026quot; value=\u0026quot;12.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_z_hit\u0026quot; value=\u0026quot;0.8\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_z_short\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_z_max\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_z_rand\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_sigma_hit\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_lambda_short\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_model_type\u0026quot; value=\u0026quot;likelihood_field\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;laser_likelihood_max_dist\u0026quot; value=\u0026quot;2.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;min_particles\u0026quot; value=\u0026quot;100\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;max_particles\u0026quot; value=\u0026quot;1000\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;kld_err\u0026quot; value=\u0026quot;0.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;kld_z\u0026quot; value=\u0026quot;0.0\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;update_min_d\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;update_min_a\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;resample_interval\u0026quot; value=\u0026quot;1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;transform_tolerance\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;recovery_alpha_slow\u0026quot; value=\u0026quot;0.001\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;recovery_alpha_fast\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_frame_id\u0026quot; value=\u0026quot;odom\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_model_type\u0026quot; value=\u0026quot;diff\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_alpha1\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_alpha2\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_alpha3\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_alpha4\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;param name=\u0026quot;odom_alpha5\u0026quot; value=\u0026quot;0.2\u0026quot;/\u0026gt; \u0026lt;/node\u0026gt; \u0026lt;/launch\u0026gt;  演習 【jetson・開発マシン】それぞれdockerコンテナを起動 jetsonでdockerコンテナを起動\n(開発PC):~$ ssh roomba_dev1 (jetson):~$ cd ~/group_a/roomba_hack (jetson)::~/group_a/roomba_hack$ git pull (jetson):~/group_a/roomba_hack$ ./RUN-DOCKER-CONTAINER.sh (jetson)(docker):~/roomba_hack# roslaunch roomba_bringup bringup.launch  開発PCでdockerコンテナを起動\n(開発PC):~$ cd ~/group_a/roomba_hack (開発PC):~/group_a/roomba_hack$ git pull (開発PC):~/group_a/roomba_hack$ ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x   gmappingで地図作成 (開発PC)(docker) roslaunch navigation_tutorial gmapping.launch  地図の保存。map.pgm（画像データ）とmap.yaml(地図情報)が保存される。\n(開発PC)(docker) rosrun map_server map_saver  ~/roomba_hack/catkin_ws/src/navigation_tutorial/map の下に保存する。\n amclをlaunchして、自己位置推定する localizationノードと地図サーバーを同時に起動。\n(開発PC)(docker) roslaunch navigation_tutorial localization.launch (開発PC)(docker) roslaunch roomba_teleop teleop.launch (開発PC)(docker) rviz -d /root/roomba_hack/catkin_ws/src/navigation_tutorial/configs/navigation.rviz   初期位置の指定(rvizの2D Pose Estimate) コントローラで移動させてみて自己位置を確認 rqt_tf_treeを見てみる   amclのparamをチューニングする launchファイルの中身を見てみて、値を変えてみる。\n各パラメータの意味はamclのページを参照。\n例えば、・・・\n initial_cov_** を大きくしてみて、パーティクルがちゃんと収束するかみてみる。 particleの数(min_particles、max_particles)を変えてみて挙動をみてみる。   launchファイルの拡張 localization.launchファイルに以下を追加してteleop.launchとrvizが同時に起動するようにしてみよう。\n\u0026lt;!-- teleop.launchを起動--\u0026gt; \u0026lt;include file=\u0026quot;$(find roomba_teleop)/launch/teleop.launch\u0026quot;\u0026gt; \u0026lt;/include\u0026gt; \u0026lt;!-- rvizを起動--\u0026gt; \u0026lt;node pkg=\u0026quot;rviz\u0026quot; type=\u0026quot;rviz\u0026quot; name=\u0026quot;navigation_rviz\u0026quot; args=\u0026quot;-d $(find navigation_tutorial)/configs/navigation.rviz\u0026quot;/\u0026gt;  ","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642809600,"objectID":"29d802971ebbc674f8dd6b80aeddda91","permalink":"https://matsuolab.github.io/roomba_hack/course/chap4/localization/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/roomba_hack/course/chap4/localization/","section":"course","summary":"","tags":null,"title":"自己位置推定","type":"book"},{"authors":null,"categories":null,"content":"ロボットシステムの開発環境に使われている要素の概要を理解する\nスライド https://docs.google.com/presentation/d/1-q6zq3vV91GTj7mw9uqwT4B8LyHDpFHBNVi4lEyCa5A/edit?usp=sharing\nLearn Linuxコマンド    command 　説明 option     ls ディレクトリ内のファイル・ディレクトリの表示 -l: 詳細を表示 -a: 全て表示   mkdir ディレクトリ作成    cd ディレクトリ移動    mv ファイル移動    rm ファイル削除 -r:ディレクトリ内を再起的に削除 -f:強制削除   cat           ssh ssh \u0026lt;username\u0026gt;@\u0026lt;hostname\u0026gt; -p \u0026lt;port\u0026gt; -i \u0026lt;identity_file\u0026gt;  エディタ  vim  チュートリアル： vimtuter   emacs  git/GitHub  gitとは  add push pull fetch clone merge reset    GitHubとは   docker   Dockerとは\n  DockerFileのビルド\ndocker build -t \u0026lt;image_name\u0026gt;:\u0026lt;tag_name\u0026gt; -f \u0026lt;Dockerfile\u0026gt; \u0026lt;relative_dir\u0026gt;    Docker Image\n# Docker image一覧 docker images # Docker Imageのダウンロード docker pull \u0026lt;image_name\u0026gt;:\u0026lt;tag_name\u0026gt; # 削除 docker rmi \u0026lt;image_id\u0026gt; # 不要なDocker imageを消す docker image prune    Docker Container\n# Docker containerの起動 docker run \u0026lt;image_name\u0026gt; \u0026lt;command\u0026gt; # Docker container一覧 docker ps -a # Docker containerに接続 docker exec -it \u0026lt;container_name\u0026gt; bash  ※docker runでよく使うオプション\n -it  標準入出力有効になる   --name \u0026lt;container_name\u0026gt;  コンテナの名前の指定   --rm  コンテナを抜けた際に自動的にコンテナを削除する   --gpus all  コンテナに全gpuを渡す gpuの個数を指定する場合は all の代わりに数字(0, 1,\u0026hellip;) gpuを指定する場合は --gpus '\u0026quot;device=0,1\u0026quot;'   -v \u0026lt;host/path/to/dir:container/path/to/dir\u0026gt;  コンテナ内にホストのディレクトリをマウントする   -p \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt;  ホストのポートをコンテナのポートにマップする コンテナ内でwebサーバを動かす場合などに使う   --net=host  コンテナとホストでネットワークを共有する(IPアドレスなどが同じになる) ROSノードをコンテナ内で動かす場合などはこれを使うと楽   --privileged  コンテナからのデバイスへのアクセスを許可 コンテナからWEBカメラにアクセスしたいときなど      演習 演習には個人PC, 開発PC, ルンバに搭載されているjetsonの3種類のコンピュータを用います。\n開発PC : robot_dev系, hsr_dev系\njetson : roomba_dev系\n【ssh】開発用PCにsshする 個人PCから開発PCにsshする\n(個人PC):~$ vim ~/.ssh/config (個人PC):~$ ssh robot_dev2  sshに成功すると\nrobot_dev2@robot-dev2:~$  などと表記が変わり、開発PCに接続できたことが確認できます。\n 【Linuxコマンド】グループのディレクトリを作成し移動する (開発PC):~$ mkdir group_a (開発PC):~$ cd group_a   【git】roomba_hackリポジトリをcloneし移動する (開発PC):~/group_a$ git clone https://github.com/matsuolab/roomba_hack.git (開発PC):~/group_a$ cd roomba_hack (開発PC):~/group_a$ ls  https://github.com/matsuolab/roomba_hack をそのままダウンロードできたことが確認できると思います。\n 【git】ブランチを確認する git branchコマンドを使ってみましょう。\n(開発PC):~/group_a/roomba_hack$ git branch   【docker】roomba_hackの開発環境のdocker imageをビルドする shellファイルを実行してビルドを行います。\n(開発PC):~/group_a/roomba_hack$ ./BUILD-DOCKER-IMAGE.sh  shellファイルの中身をcatコマンドで確認してみましょう。\n(開発PC)2:~/group_a/roomba_hack$ cat BUILD-DOCKER-IMAGE.sh  細かいところは気にしなくていいですが、ファイルの最後の\ndocker build . -f docker/${DOCKERFILE_NAME} -t ${IMAGE_NAME}:${TAG_NAME} --build-arg BASE_IMAGE=${BASE_IMAGE}  でビルドが行われていることが確認できると思います。\n 【ssh】jetsonにsshする 開発用PCからルンバに載っているjetson nanoへsshします。\n(開発PC):~/group_a/roomba_hack$ ssh roomba_dev2 roomba_dev2@roomba-dev-jetson2:~$  先頭の表記がroomba_dev2@roomba-dev-jetson2と変わり、jetsonへ接続されたことがわかります。\njetsonでも同様にグループのディレクトリを作成し、移動し、roomba_hackリポジトリをcloneしてみましょう。\n 【ssh】VNCを使う 個人PCから開発PCにsshで接続\n(個人PC):~$ ssh robot_dev2 -L 5900:localhost:5900  手元のVNC viewerでlocalhost:5900を開く\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"2a1ada4f20c1997bdd8592ad8bf5f9b1","permalink":"https://matsuolab.github.io/roomba_hack/course/chap1/%E9%96%8B%E7%99%BA%E7%92%B0%E5%A2%83/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/roomba_hack/course/chap1/%E9%96%8B%E7%99%BA%E7%92%B0%E5%A2%83/","section":"course","summary":"ロボットシステムの開発環境に使われている要素の概要を理解する\n","tags":null,"title":"開発環境","type":"book"},{"authors":null,"categories":null,"content":"ROSのパッケージ管理について理解しよう\nLearn ROSのパッケージ ROSでは、特定の目的のためのプログラム群をまとめてパッケージとして管理する。\n例として、navigation_tutorialパッケージのファイル構成を示す。\nnavigation_tutorial ├── CMakeLists.txt ├── launch │ ├── amcl.launch │ ├── avoidance.launch │ ├── gmapping.launch │ ├── go_straight.launch │ ├── localization.launch │ ├── map_server.launch │ ├── move_base.launch │ └── navigation.launch ├── package.xml ├── params │ ├── base_global_planner_params.yaml │ ├── base_local_planner_params.yaml │ ├── costmap_common_params.yaml │ ├── dwa_local_planner_params.yaml │ ├── global_costmap_params.yaml │ ├── local_costmap_params.yaml │ └── move_base_params.yaml ├── scripts │ ├── avoidance.py │ ├── simple_control2.py │ └── simple_control.py └── src ├── avoidance.cpp └── go_straight.cpp  一般的に、scriptsディレクトリやsrcディレクトリにそれぞれPython, C++のプログラムが配置される。\n作成したプログラムはrosrunコマンドで実行することができる。\n(Python) rosrun navigation_tutorial simple_control2.py (C++) rosrun navigation_tutorial go_straight  launchディレクトリに入っているlaunchファイルは複数のプログラムを同時に実行できるための仕組みである。\nlaunchファイルについてでも同様にroslaunchコマンドで実行することができる。\nroslaunch navigation_tutorial move_base.launch  実行時にパッケージ名(今回だとnavigation_tutorial)を指定するので、現在どこのディレクトリにいるかに関係なく実行が可能である。\nROSのワークスペース ROSのパッケージはワークスペースと呼ばれる作業スペースに配置される。\n一般的にcatkin_wsという名前が使われることが多い。\ncatkin_wsのファイル構成を示す。\ncatkin_ws ├── build ├── devel └── src ├── CMakeLists.txt ├── navigation_tutorial │ ├── CMakeLists.txt │ ├── launch │ ├── package.xml │ ├── params │ ├── scripts │ └── src └── roomba ├── roomba_bringup │ ├── CMakeLists.txt │ ├── config │ ├── launch │ └── package.xml ├── roomba_description │ ├── CMakeLists.txt │ ├── config │ ├── launch │ ├── meshes │ ├── package.xml │ └── urdf ├── roomba_gazebo │ ├── CMakeLists.txt │ ├── launch │ └── package.xml └── roomba_teleop ├── CMakeLists.txt ├── include ├── launch ├── package.xml └── src  catkin_wsのsrc内でパッケージ作成を行い、catkin_ws直下でcatkin_makeコマンドを実行すると、Cプログラムのビルドが行われ、buildディレクトリとdevelディレクトリが作成される。\n作成されたdevelディレクトリの中のsetup.bashをソースsource devel/setup.bashすることで、ワークスペース内のパッケージのパスを通すことができる。　パッケージのパスを通すことで、ROSのパッケージに関するコマンドや、プログラムの実行(rosrunやroslaunch)が行えるようになる。\nROSのコマンド ROSのコマンドのうち、よく用いるものを紹介する。\n Topic関連  rostopic list topicの一覧を表示する rostopic echo \u0026lt;topic name\u0026gt; 指定されたtopicの中身を表示する rostopic hz \u0026lt;topic name\u0026gt; topicの配信周波数を取得する rostopic info \u0026lt;topic name\u0026gt; topicの情報を表示する rostopic pub \u0026lt;topic name\u0026gt; \u0026lt;topic\u0026gt; topicを配信する rostopic type \u0026lt;topic name\u0026gt; topicの型を確認する   Node関連  rosnode list nodeの一覧を表示する rosnode ping \u0026lt;node name\u0026gt; nodeの接続テストを行う rosnode info \u0026lt;node name\u0026gt; nodeの情報を表示する rosnode kill \u0026lt;node name\u0026gt; nodeをシャットダウンする   Package関連  rospack list packageの一覧を表示する roscd \u0026lt;package name\u0026gt; 指定したpackage内に移動する  ROSのプログラムの書き方 それでは実際にプログラム例を見てみましょう。\n#!/usr/bin/env python3 import rospy from geometry_msgs.msg import Twist def time_control(pub, velocity, yawrate, time): vel = Twist() start_time = rospy.get_rostime().secs while(rospy.get_rostime().secs-start_time\u0026lt;time): vel.linear.x = velocity vel.angular.z = yawrate pub.publish(vel) rospy.sleep(0.1) def simple_controller(): rospy.init_node('simple_controller', anonymous=True) pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10) time_control(pub, 0.0, 0.0, 0.5) time_control(pub, 0.3, 0.0, 2.0) time_control(pub, 0.0, 0.0, 0.5) time_control(pub, -0.3, 0.0, 2.0) time_control(pub, 0.0, 0.0, 0.5) time_control(pub, 0.0, 0.5, 2.0) time_control(pub, 0.0, 0.0, 0.5) time_control(pub, 0.0, -0.5, 2.0) if __name__=='__main__': try: simple_controller() except rospy.ROSInitException: pass  まずsimple_controller関数内をみていきましょう。\n以下の部分でsimple_controllerという名前でノードを定義しています。\nrospy.init_node('simple_controller', anonymous=True)  以下の部分でPublisher(トピックのpublish)を宣言しています。\n今回の場合は、/cmd_velトピックをTwist型で送信するPublisherを宣言しています。\npub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)  続いて、time_control関数です。\nこの関数はpublisher、速度、角速度、時間を受け取り、速度指令をpublishします。\ndef time_control(pub, velocity, yawrate, time): vel = Twist() start_time = rospy.get_rostime().secs while(rospy.get_rostime().secs-start_time\u0026lt;time): vel.linear.x = velocity vel.angular.z = yawrate pub.publish(vel) rospy.sleep(0.1)  ここでTwist型のインスタンスを作成しています。\nvel = Twist()  while文で受け取った時間が過ぎるまでの間、受け取った速度と各速度をvelに格納し、pub.publish(vel)でpublishを行なっています。\nwhile(rospy.get_rostime().secs-start_time\u0026lt;time): vel.linear.x = velocity vel.angular.z = yawrate pub.publish(vel) rospy.sleep(0.1)  演習 【jetson・開発マシン】それぞれdockerコンテナを起動 jetsonでdockerコンテナを起動\n(開発PC):~$ ssh roomba_dev1 (jetson):~$ cd ~/group_a/roomba_hack (jetson):~/group_a/roomba_hack ./RUN-DOCKER-CONTAINER.sh (jetson)(docker):~/roomba_hack#  開発PCでdockerコンテナを起動\n(開発PC):~$ cd ~/group_a/roomba_hack (開発PC):~/group_a/roomba_hack ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x (開発PC)(docker):~/roomba_hack#   【jetson・開発マシン】ビルドをしてパスを通す catkin_make後にdevelとbuildディレクトリが作成されることを確認しましょう。\n(開発PC)(docker):~/roomba_hack# cd catkin_ws (開発PC)(docker):~/roomba_hack/catkin_ws# rm -rf devel build (開発PC)(docker):~/roomba_hack/catkin_ws# ls (開発PC)(docker):~/roomba_hack/catkin_ws# catkin_make (開発PC)(docker):~/roomba_hack/catkin_ws# ls (開発PC)(docker):~/roomba_hack/catkin_ws# source ./devel/setup.bash   【jetson】ROSマスタ、各種ノードを起動 (jetson)(docker):~/roomba_hack# roslaunch roomba_bringup bringup.launch   ROSメッセージの可視化 【開発PC】topicの確認 Topic関連のコマンドのところのrostopic listコマンドを使用してtopic一覧を表示してみましょう\n(開発PC)(docker):~/roomba_hack# rostopic list  特定のtopicの型を確認\n(開発PC)(docker)# rostopic type /camera/color/image_raw (開発PC)(docker)# rostopic type /scan  その型が実際にどのような構成をしているのかはrosmsg info \u0026lt;topic type\u0026gt;で調べられます。\n参考\nsensor_msgs/LaserScan型 http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/LaserScan.html\nsensor_msgs/Image型 http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html\n特定のtopicの中身を確認\n(開発PC)(docker)# rostopic echo /camera/color/image_raw (開発PC)(docker)# rostopic echo /scan  rvizを用いて可視化\n(開発PC)(docker)# rviz   【開発PC】topicのpublish(配信) topic/cmd_velの情報を確認\n(開発PC)(docker)# rostopic info /cmd_vel  topic/cmd_velの型を確認\n(開発PC)(docker)# rostopic type /cmd_vel  geometry_msgs/Twist型 http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Twist.html\ntopic/cmd_velをpublish\n(開発PC)(docker)# rostopic pub /cmd_vel geometry_msgs/Twist \u0026quot;linear: x: 1.0 y: 0.0 z: 0.0 angular: x: 0.0 y: 0.0 z: 0.0\u0026quot;  (開発PC)(docker)# rosrun navigation_tutorial simple_control.py   Try it! 時間が余った人向け try it! roomba_bringupパッケージのbringup.launchの中身を読んでみよう\nhint roscdコマンドを使うとパッケージへ簡単に移動ができます。ファイルの中身を表示するにはcatコマンドを使用します。\ntry it! 開発PCでrosnode関連のコマンドを使ってみよう\ntry it! 開発PCでrosrun rqt_graph rqt_graphを実行してnodeとtopicの関連を可視化してみよう\ntry it! 開発PCでsimple_control.pyの中身を読んでコードを変更してみよう\nhint コードを編集するときはエディタを使うことがおすすめです。新しくターミナルを開いて\n(開発PC):~$ cd group_a/roomba_hack (開発PC):~group_a/roomba_hack$ code .  でVScodeを起動することができます。\n","date":1649116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649116800,"objectID":"3a3eac87851dbc29e0c5e1d274ba3f31","permalink":"https://matsuolab.github.io/roomba_hack/course/chap2/rosbasic/","publishdate":"2022-04-05T00:00:00Z","relpermalink":"/roomba_hack/course/chap2/rosbasic/","section":"course","summary":"ROSのパッケージ管理について理解しよう\n","tags":null,"title":"ROSのパッケージ・ワークスペース","type":"book"},{"authors":null,"categories":null,"content":"センサの値を読み取りロボットを動かしてみよう\nLearn ロボットセンサの基礎知識 ロボットが動作するために必要なセンサは大きく2種類に分けられる。\n1つ目が外界センサで、これはロボットが行動する環境の情報を取得するためのセンサーである。 具体的なセンサとして、\n LiDAR デプスカメラ ホイールエンコーダ IMU  などがあげられる。\nセンサのノイズの影響を軽減するため、複数のセンサを組み合わせて利用されることもある。\n2つ目は内界センサで、これは(ロボットアームのような変形可能な)ロボットが自身の内部状態を把握し、位置や姿勢を制御するために使われるセンサーである。\n 関節位置・角度センサ 関節姿勢センサ  などが内界センサである。\n参考\n https://www.jsme.or.jp/jsme-medwiki/14:1013897#:~:text=robot%20sensor ","date":1649116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649116800,"objectID":"ff51cff84b07cbb120bc73f6c20e4a6b","permalink":"https://matsuolab.github.io/roomba_hack/course/chap2/sensing1/","publishdate":"2022-04-05T00:00:00Z","relpermalink":"/roomba_hack/course/chap2/sensing1/","section":"course","summary":"センサの値を読み取りロボットを動かしてみよう\n","tags":null,"title":"ロボットシステムにおけるセンシング・アクチュエーション・通信①","type":"book"},{"authors":null,"categories":null,"content":"複数のセンサを組み合わせてよりかしこくロボットを動かしてみよう\nLearn LiDARのスキャンデータを使って，障害物を回避してみよう 次に，LiDARでスキャンしたデータを使って，障害物を回避するようなプログラムを作ってみましょう．\nLiDARスキャンのメッセージ（/scan）の中身を見てみよう LiDARは，Light Detection And Rangingの略で，レーザ光を使って離れた場所にある物体形状や距離を測定するためのセンサです． 近年では，自動車の自動運転にも用いられることの多いセンサの一つです．\nroombaに搭載されたLiDARセンサ（rplidar）の値は，/scanのトピックに流れていて，rostopic echo /scanをしてみるとメッセージとしてどんな情報が流れているかわかります．\n大きなデータなので今回はテキストに掲載するのは省略しますが，rostopic type /scanをしてみると，メッセージとして，sensor_msgs/LaserScan型が使われていることがわかります． rostopic type /scan root@dynamics:~/roomba_hack# rostopic type /scan sensor_msgs/LaserScan  \nsensor_msgs/LaserScan型の定義を確認してみましょう． メッセージ型の定義は，ドキュメントのほか，rosmsg info sensor_msgs/LaserScanすることでもコマンドから確認できます． rosmsg info sensor_msgs/LaserScan root@dynamics:~/roomba_hack# rosmsg info sensor_msgs/LaserScan std_msgs/Header header uint32 seq time stamp string frame_id float32 angle_min float32 angle_max float32 angle_increment float32 time_increment float32 scan_time float32 range_min float32 range_max float32[] ranges float32[] intensities  \nangle_minにはスキャンの開始角度，angle_maxにはスキャンの終了角度がラジアンで記録されています． angle_incrementは，計測した間隔がラジアンで記録されています． range_maxにはスキャンの間で検出された最大の距離，range_minには最小の距離がメートルで記録されています．\nrvizでLiDARスキャンの値を可視化してみよう rvizでLiDARのスキャン結果を可視化してみましょう．\nLaserScanをAddして，topicに/scanを設定すると，以下のように，ロボットを中心にLiDARによって計測された障害物が赤く表示されます．\n  LiDARスキャンをrvizで可視化  LiDARを使って障害物を回避しよう それでは，LiDARスキャン/scenの情報を使った制御の実装の例としてnavigation_tutorialパッケージの中のavoidance.pyを見てみましょう（github）．\nこのプログラムでは，LiDARを使って進行方向に存在する障害物を見つけ，それを回避しながら進むようにロボットを制御しています．具体的には，\n ロボットの進行方向に物体がなかったら直進 ロボットの右側に障害物があったら左回転 ロボットの左側に障害物があったら右回転  することで障害物を回避（ぶつかる前に方向転換）しています．\nでは，プログラムの中身を見ていきます．\n/odomを使った制御の場合と同様に，ノードを定義する際に，コマンドを送るパブリッシャと，LiDARスキャンのデータを読み取るサブスクライバを作成します．\nclass Avoidance: def __init__(self): rospy.init_node('avoidance', anonymous=True) # Publisher self.cmd_vel_pub = rospy.Publisher('/planner/cmd_vel', Twist, queue_size=10) # Subscriber scan_sub = rospy.Subscriber('/scan', LaserScan, self.callback_scan) self.min_range = None  /scanのコールバックは，\ndef callback_scan(self, data): fov = np.deg2rad(60) min_range = data.range_max min_idx = -1 angle = data.angle_min for idx, r in enumerate(data.ranges): angle += data.angle_increment if -fov\u0026lt;angle\u0026lt;fov: if r\u0026lt;min_range: min_range = r min_idx = idx if min_idx \u0026lt; len(data.ranges)/2.0: self.direction = \u0026quot;RIGHT\u0026quot; else: self.direction = \u0026quot;LEFT\u0026quot; self.min_range = min_range  となっており，正面から左右60度の範囲内で最も短い距離をself.min_rangeに格納し，それが右側にあるのか左側にあるのかをself.directionに格納しています．．\nこのプログラムを実行するとprocessメソッドが（0.1秒おきに）常に実行されます．\ndef process(self): r = rospy.Rate(10) while not rospy.is_shutdown(): vel = Twist() if self.min_range is not None: if self.min_range \u0026gt;= 0.4: vel.linear.x = 0.2 vel.angular.z = 0.0 else: vel.linear.x = 0.0 if self.direction == \u0026quot;RIGHT\u0026quot;: vel.angular.z = 0.5 elif self.direction == \u0026quot;LEFT\u0026quot;: vel.angular.z = -0.5 self.cmd_vel_pub.publish(vel) r.sleep()  processメソッド内部では，格納されたself.min_rangeが0.4（メートル）より大きい場合は，ロボットの前に何もないと判断して直進，小さい場合は，self.directionの値を見て，RIGHTであれば右に障害物があると判断して左回転，LEFTであれば左に障害物があると判断して右回転するようなプログラムになっています．\nそれでは，実際にLiDARを使って障害物を回避するプログラムを実行してみましょう．\n演習 ROSメッセージの可視化 【開発PC】topicの確認 /scanの型を確認\n(開発PC)(docker):~/roomba_hack# rostopic type /scan  /scanの中身を確認\n(開発PC)(docker):~/roomba_hack# rostopic echo /scan   LiDARスキャンを使ったフィードバック制御 avoidance.pyを実行してみよう．\nこのプログラムを動かすときには，コントローラのYボタンを押してからBボタンを押してautoモードにしておきましょう．\n今回はせっかくなので，launchfileから起動してみましょう． このlaunchfileは，navigation_tutorialパッケージの中のlaunchフォルダの中にあるavoidance.launchに記述されています（github）．\n(開発PC)(docker):~/roomba_hack# roslaunch navigation_tutorial avoidance.launch  ロボットの進行方向に障害物があるときに，それを避けるように方向転換したら成功です．\ntry it! avoidance.pyの中身を読んでコードを変更してみよう\n","date":1649116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649116800,"objectID":"4115bb19a4232d2372987cbff0c843fc","permalink":"https://matsuolab.github.io/roomba_hack/course/chap3/sensing3/","publishdate":"2022-04-05T00:00:00Z","relpermalink":"/roomba_hack/course/chap3/sensing3/","section":"course","summary":"複数のセンサを組み合わせてよりかしこくロボットを動かしてみよう\n","tags":null,"title":"ロボットシステムにおけるセンシング・アクチュエーション・通信③","type":"book"},{"authors":null,"categories":null,"content":"ロボット開発によく用いられるROSの概要を理解する\nLearn ROSの概要 ROS(Robot Operating System)は、ロボット・アプリケーション作成を支援するライブラリとツールを提供するミドルウェアです。 具体的には以下にあげるものをROSは提供しています。\n  メッセージ通信\nプロセス間、コンピュータ間の通信ライブラリが提供されています。用途に応じて、多対多や一対多、非同期、同期などの通信形態を選択することができます。\n  デバイスドライバ\nロボットに搭載される多くのセンサやアクチュエータがROSのAPIで標準化された形で提供されています。\nhttps://github.com/ros-drivers http://wiki.ros.org/Sensors\n  ライブラリ\nロボットを動作させるソフトウェア(ナビゲーション、マニピュレーション)の基本機能の大半が提供されています。\n  視覚化ツール\nロボットの内部状態やセンサ出力を2次元、3次元で視覚化するRvizや3次元動力学シミュレータのGazeboなどが提供されています。\n  パッケージ管理\n多種多様なプログラミング言語(python, C++, \u0026hellip;)、依存関係で記述されたプログラム(パッケージ)同士を統合的にセットアップ、ビルド、テスト、リリースすることが可能です。\nたとえば、経路計画など処理が重いプロセスはC++で、画像認識など機械学習系のプロセスはpythonで実装し、それらプロセス間の通信を容易に実装できる。\n  ROSのメッセージ通信 ロボットシステムでは、多数のプログラムを並列に実行し、それぞれがデータをやりとりします。 それらのプログラム間の通信ライブラリをROSは提供します。\n  ノード(node)\nROSでは、一つのプログラム単位を「ノード(node)」と呼びます。 ノードは、ROSクライアントライブラリを用いて、他のノードとデータをやりとりします。 ROSクライアントライブラリは異なるプログラミング言語で記述されたノードがやりとりできるようにしています。 ノードは、次に述べるトピックの配信・購読、またはサービスの提供・使用が可能です。\n  トピック(topic)\nROSでの、標準的なデータ通信の経路を「トピック(topic)」と呼びます。 ノードはメッセージをトピックへ向けて配信(Publish)し、同様に購読する(Subscribe)ことでトピックからメッセージを受け取ることができます。\nトピックには名前が付けられ、同じトピックに複数のノードがデータを送り、複数のノードがデータを受け取ることができます。\n  メッセージ(message)\nトピックへ配信したり、購読したりするときのROSのデータ型のことを「メッセージ(message)」と呼びます。 メッセージの型はmsgファイルに記述されており、使用言語に依存しないデータ形式になっています。\n以下に、物体やロボットの位置を表す時によく用いるgeomemtry_msgs/PoseStamped型のmsgファイルを示します。 位置情報の時間や座標フレームの情報が含まれるheaderと座標位置を表すposeで定義されています。\nstd_msgs/Header header uint32 seq time stamp string frame_id geometry_msgs/Pose pose geometry_msgs/Point position float64 x float64 y float64 z geometry_msgs/Quaternion orientation float64 x float64 y float64 z float64 w    サービス(service)\n「サービス(service)」はノードが他のノードとお互いに通信するための一つの手段です。 サービスを提供しているノードに引数を渡して、関数の実行結果を戻り値として受け取ることができます。\n呼び出される側のノードは、サービス名とデータ形式の宣言を「アドバタイズ(advertise)」し、呼び出す側のノードは、サービスを「コール(call)」します。\nサービスにおいて送受信されるデータの型はsrvファイルに記述されています。 メッセージと同様使用言語に依存しないデータ形式ですが、メッセージと異なるのは、引数と戻り値の二つの形式を定義する必要があるところです。\n以下に、srvの例としてstd_srvs/SetBoolを示します。 このように引数と戻り値の間に---を入れて定義します。\nbool data --- bool success string message    ROSマスタ(ROS master)\n「ROSマスタ(ROS master)」は、ノード、トピックおよびサービスの名前登録を行い、それぞれのノードが他のノードから見えるようにする役割を担っています。 通信するノード名とトピック名およびサービス名の対応が決定した後、ノード同士が「peer-to-peer」で通信します。\nROSマスタとノード間の通信はXML-RPCを用いて行われます。 ROSマスタを起動するには「roscore」というコマンドを実行します。\n  パラメータサーバ(parameter server)\n「パラメータサーバ(parameter server)」は、設定データを複数のノードで共有するための軽量なサーバです。 各ノードのパラメータを、パラメータサーバで一括して管理できます。 パラメータサーバもROSマスタ同様に「roscore」コマンドで起動します。\nパラメータサーバで扱える型は、整数・小数・真偽値・辞書・リストになります。\n  ROSのデータ通信のまとめ\n    ROS通信  ROSと連動するソフトウェア ROSは以下のソフトウェアと連動して使うためのパッケージが提供されています。\n  OpenCV\nコンピュータビジョンの標準的なライブラリ。\nOpenCVのデータ形式である、MatクラスとROSのメッセージ形式を変換するcv_bridgeや３次元座標上の物体を２次元画像上に投影する機能であるimage_geometryといったパッケージ(vision_opencv)が提供されています。\n  PCL(Point Cloud Library)\n3次元点群処理のライブラリ。\nOpenCV同様PCLのデータ形式とROSのメッセージ形式を変換するパッケージが提供されています。\n  OpenSLAM, Navigation Stack\n移動ロボットの自己位置推定と地図生成を同時に行うSLAM(Simultaneous Localization and Mapping)のソースコードを公開するためのプラットフォームと、。\nROSではOpenSLAMで実装されているgmappingパッケージのラッパーやそれと連携して自律走行を実現するnavigationメタパッケージが提供されています。\n  Move it\n  視覚化ツール  rqt    rqt window   rviz     gazebo  演習 roomba driverを起動し、動作していることを確認する   jetsonにアクセスする\n(開発PC):~$ ssh roomba_dev1 (jetson):~$    docker containerを起動する 余裕があればRUN-DOCKER-CONTAINER.shファイルの中身を確認してみましょう。\n(jetson):~$ cd ~/team_a/roomba_hack (jetson):~/team_a/roomba_hack$ ./RUN-DOCKER-CONTAINER.sh root@roomba-dev-jetson:~/roomba_hack#  root@roomba-dev-jetson:~/roomba_hack#などと表示されればdocker内部に入れています。\n今後docker内部であることは(docker)と表記します。\n  roomba driverなどを起動するlaunchファイルを起動する このタイミングでルンバの電源が入っているかを確認しておきましょう。\n(jetson)(docker):~/roomba_hack# roslaunch roomba_bringup bringup.launch  起動に成功すればルンバからピッと短い音が鳴り、ターミナルには赤い文字が出続けるはずです。\n   コントローラーを使って、ロボットを動かす   開発PCでdocker containerを起動する xにはroomba_devの後につく数字を入れてください。\n(開発PC):~$ cd ~/team_a/roomba_hack (開発PC):~/team_a/roomba_hack$ ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x    コントローラーを起動 コントローラーが開発PCに刺さってることを確認してください。\n(開発PC)(docker):~/roomba_hack# cd catkin_ws (開発PC)(docker):~/roomba_hack/catkin_ws# catkin_make (開発PC)(docker):~/roomba_hack/catkin_ws# source devel/setup.bash (開発PC)(docker):~/roomba_hack/catkin_ws#roslaunch roomba_teleop teleop.launch    コントローラのモード\n 移動・停止 自動・マニュアル ドッキング・アンドッキング    コントローラによる操縦\n 移動ロック解除 L1を押している時のみ移動コマンドが動作します。 左ジョイスティック 縦方向で前進速度(手前に倒すとバック)、横方向は回転速度に対応しています。 左矢印 それぞれ、一定に低速度で前進・後退・回転します。    正常に起動できているかを確認 開発PCで新しくターミナルを開いてdockerの中に入ります。\nすでに開発PCで起動されているdockerコンテナに入る場合は、\n(開発PC):~/group_a/roomba_hack$ docker exec -it roomba_hack bash  または\n(開発PC):~/group_a/roomba_hack$ ./RUN-DOCKER-CONTAINER.sh  のいずれかの方法で入ることができます。\nさまざまなコマンドを使ってroombaの情報を取得してみましょう。\n(開発PC)(docker):~/roomba_hack# rosnode list (開発PC)(docker):~/roomba_hack# rostopic list (開発PC)(docker):~/roomba_hack# rostopic echo /cmd_vel (開発PC)(docker):~/roomba_hack# rqt_graph (開発PC)(docker):~/roomba_hack# rviz    ","date":1642809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642809600,"objectID":"20855653654f6e28e5120898d90797bd","permalink":"https://matsuolab.github.io/roomba_hack/course/chap1/ros/","publishdate":"2022-01-22T00:00:00Z","relpermalink":"/roomba_hack/course/chap1/ros/","section":"course","summary":"ロボット開発によく用いられるROSの概要を理解する\n","tags":null,"title":"ROSとは","type":"book"},{"authors":null,"categories":null,"content":"Learn Navigationシステム ナビゲーションは、地図上の任意の目標地点へ、障害物を避けながらなるべく早く自律して移動することが目的です。\nナビゲーションシステムの出力はロボットへの行動指令値(速度など)ですが、入力は以下の4つになります。\n 地図 目標位置 自己位置推定結果 リアルタイムのセンサ情報(LiDARスキャン情報など)  ナビゲーションでは、地図全体とロボット周辺(センサで見える範囲)の大きく2つに分けて考えることが多いです。\n地図全体を考えるグローバルパスプランでは、地図情報とゴール情報から大まかなゴールまでの経路を算出します。\nロボット周辺を考える ローカルパスプランでは、グローバルで算出した経路に沿うようにしつつ、周辺の障害物情報を避ける行動指令値を算出します。\nそれぞれの経路を考えるにあたって、経路のコストがどうなるか重要になります。 このコストを表現する方法として、コストマップが用いられることが多いです。\n  Navigationシステム概要(from ROS wiki)  Cost Map コストマップは、経路を算出するために用いることから、扱いやすいグリット上の占有格子地図という形で表現されることが多いです。\n(空を飛んだり、3次元地形を考えなくていい場合は、基本2次元で表現します。)\n経路は格子地図上で、点で扱うことが多いですが、ロボット自身はある程度の大きさを持っているので、スキャン情報で得られた点ギリギリに経路を生成すると、衝突してしまします。\nそのため、コストマップでは以下の図のようにスキャンで得られた点(図中の赤点)から、ロボットが入ってほしくない範囲にコスト(図中の青く塗りつぶされているところ)が付与するという表現をします。\n  コストマップ概要(from ROS wiki)  Global Path Planning グローバルパスプランの例として、グラフ探索を利用したダイクストラ法やA*法などで経路探索をすることがあります。\n  グローバルパスプランの例(from PythonRobotics)  Local Path Planning 局所経路計画(Local Path Planning)は、ロボット周辺の障害物を避けながら、目標値へ早く行けるような経路(ロボットの行動)を算出するモジュールです。\n代表的なアルゴリズムとしてDynamic Window Approach(DWA)というものがあります。   ローカルパスプラン概要(from ROS wiki) \nアルゴリズムの概要は以下になります。\n ロボットの行動空間から行動をサンプル サンプルした行動とロボットの運動モデルを用いて、一定時間シミュレーションをして経路を生成 生成した経路ごとに、コストマップやゴール情報からコストを算出 コスト最小の経路を選択し、ロボットの指令値とする 1~4を繰り返す  演習 Dockerfileにnavigationを追加してBuildする \n -- navigationをlaunchして、rviz上で指定した位置までナビゲーションさせてみる (開発PC)(docker) roslaunch navigation_tutorial navigation.launch   navigationをlaunchして、map座標系の位置を指定してナビゲーションさせてみる \n -- navigationのparamをチューニングする move baseのパラメータは navigation_tutorial/params の中にyaml形式で保存されています。\nlaunchファイルではloadコマンドでyamlを読み込んでいます。\n move_base base_local_planner costmap_2d  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"ea25624150d5f3e46f1ef192419e7583","permalink":"https://matsuolab.github.io/roomba_hack/course/chap4/navigation/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/roomba_hack/course/chap4/navigation/","section":"course","summary":"","tags":null,"title":"ナビゲーション","type":"book"},{"authors":null,"categories":null,"content":"Congratulations to Jian Yang and Monica Hall for winning the Best Paper Award at the 2020 Conference on Wowchemy for their paper “Learning Wowchemy”.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer tempus augue non tempor egestas. Proin nisl nunc, dignissim in accumsan dapibus, auctor ullamcorper neque. Quisque at elit felis. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget elementum odio. Cras interdum eget risus sit amet aliquet. In volutpat, nisl ut fringilla dignissim, arcu nisl suscipit ante, at accumsan sapien nisl eu eros.\nSed eu dui nec ligula bibendum dapibus. Nullam imperdiet auctor tortor, vel cursus mauris malesuada non. Quisque ultrices euismod dapibus. Aenean sed gravida risus. Sed nisi tortor, vulputate nec quam non, placerat porta nisl. Nunc varius lobortis urna, condimentum facilisis ipsum molestie eu. Ut molestie eleifend ligula sed dignissim. Duis ut tellus turpis. Praesent tincidunt, nunc sed congue malesuada, mauris enim maximus massa, eget interdum turpis urna et ante. Morbi sem nisl, cursus quis mollis et, interdum luctus augue. Aliquam laoreet, leo et accumsan tincidunt, libero neque aliquet lectus, a ultricies lorem mi a orci.\nMauris dapibus sem vel magna convallis laoreet. Donec in venenatis urna, vitae sodales odio. Praesent tortor diam, varius non luctus nec, bibendum vel est. Quisque id sem enim. Maecenas at est leo. Vestibulum tristique pellentesque ex, blandit placerat nunc eleifend sit amet. Fusce eget lectus bibendum, accumsan mi quis, luctus sem. Etiam vitae nulla scelerisque, eleifend odio in, euismod quam. Etiam porta ullamcorper massa, vitae gravida turpis euismod quis. Mauris sodales sem ac ultrices viverra. In placerat ultrices sapien. Suspendisse eu arcu hendrerit, luctus tortor cursus, maximus dolor. Proin et velit et quam gravida dapibus. Donec blandit justo ut consequat tristique.\n","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"2a0ec8a990dbd78a00c4e15a09364b00","permalink":"https://matsuolab.github.io/roomba_hack/post/20-12-02-icml-best-paper/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/roomba_hack/post/20-12-02-icml-best-paper/","section":"post","summary":"Congratulations to Jian Yang and Monica Hall for winning the Best Paper Award at the 2020 Conference on Wowchemy for their paper “Learning Wowchemy”.\n","tags":null,"title":"Jian Yang and Monica Hall Win the Best Paper Award at Wowchemy 2020","type":"post"},{"authors":null,"categories":null,"content":"Congratulations to Richard Hendricks for winning first place in the Wowchemy Prize.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer tempus augue non tempor egestas. Proin nisl nunc, dignissim in accumsan dapibus, auctor ullamcorper neque. Quisque at elit felis. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget elementum odio. Cras interdum eget risus sit amet aliquet. In volutpat, nisl ut fringilla dignissim, arcu nisl suscipit ante, at accumsan sapien nisl eu eros.\nSed eu dui nec ligula bibendum dapibus. Nullam imperdiet auctor tortor, vel cursus mauris malesuada non. Quisque ultrices euismod dapibus. Aenean sed gravida risus. Sed nisi tortor, vulputate nec quam non, placerat porta nisl. Nunc varius lobortis urna, condimentum facilisis ipsum molestie eu. Ut molestie eleifend ligula sed dignissim. Duis ut tellus turpis. Praesent tincidunt, nunc sed congue malesuada, mauris enim maximus massa, eget interdum turpis urna et ante. Morbi sem nisl, cursus quis mollis et, interdum luctus augue. Aliquam laoreet, leo et accumsan tincidunt, libero neque aliquet lectus, a ultricies lorem mi a orci.\nMauris dapibus sem vel magna convallis laoreet. Donec in venenatis urna, vitae sodales odio. Praesent tortor diam, varius non luctus nec, bibendum vel est. Quisque id sem enim. Maecenas at est leo. Vestibulum tristique pellentesque ex, blandit placerat nunc eleifend sit amet. Fusce eget lectus bibendum, accumsan mi quis, luctus sem. Etiam vitae nulla scelerisque, eleifend odio in, euismod quam. Etiam porta ullamcorper massa, vitae gravida turpis euismod quis. Mauris sodales sem ac ultrices viverra. In placerat ultrices sapien. Suspendisse eu arcu hendrerit, luctus tortor cursus, maximus dolor. Proin et velit et quam gravida dapibus. Donec blandit justo ut consequat tristique.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"be2bd15f022f0d83fe9ffd743881e70c","permalink":"https://matsuolab.github.io/roomba_hack/post/20-12-01-wowchemy-prize/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/roomba_hack/post/20-12-01-wowchemy-prize/","section":"post","summary":"Congratulations to Richard Hendricks for winning first place in the Wowchemy Prize.\n","tags":null,"title":"Richard Hendricks Wins First Place in the Wowchemy Prize","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://matsuolab.github.io/roomba_hack/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/roomba_hack/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://matsuolab.github.io/roomba_hack/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/roomba_hack/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]