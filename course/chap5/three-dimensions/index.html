<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="TRAIL Admin"><meta name=description content><link rel=alternate hreflang=ja href=https://matsuolab.github.io/roomba_hack_course/course/chap5/three-dimensions/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#4caf50"><script src=/roomba_hack_course/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/roomba_hack_course/css/wowchemy.e6acd01addd9a75ba87b4d40035795b4.css><link rel=manifest href=/roomba_hack_course/index.webmanifest><link rel=icon type=image/png href=/roomba_hack_course/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/roomba_hack_course/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://matsuolab.github.io/roomba_hack_course/course/chap5/three-dimensions/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="ロボットシステム入門"><meta property="og:url" content="https://matsuolab.github.io/roomba_hack_course/course/chap5/three-dimensions/"><meta property="og:title" content="三次元画像処理 | ロボットシステム入門"><meta property="og:description" content><meta property="og:image" content="https://matsuolab.github.io/roomba_hack_course/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://matsuolab.github.io/roomba_hack_course/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2022-01-22T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-22T00:00:00+00:00"><title>三次元画像処理 | ロボットシステム入門</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=6efadb7100ad083ad0bc43c5fd3c3b1a><script src=/roomba_hack_course/js/wowchemy-init.min.b8153d4570dcbb34350a2a846dba8c03.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/roomba_hack_course/>ロボットシステム入門</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/roomba_hack_course/>ロボットシステム入門</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/roomba_hack_course/course><span>Courses</span></a></li><li class=nav-item><a class=nav-link href=/roomba_hack_course/post><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/roomba_hack_course/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 docs-sidebar"><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Chapter 5</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>Search...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/>ロボットシステム入門</a></li><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap1/><i class="fas fa-book pr-1"></i>Chapter 1</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/chap1/%E9%96%8B%E7%99%BA%E7%92%B0%E5%A2%83/>開発環境</a></li><li><a href=/roomba_hack_course/course/chap1/ros/>ROSとは</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap2/><i class="fas fa-book pr-1"></i>Chapter 2</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/chap2/rosbasic/>ROSのパッケージ・ワークスペース</a></li><li><a href=/roomba_hack_course/course/chap2/sensing1/>ロボットシステムにおけるセンシング・アクチュエーション・通信①</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap3/><i class="fas fa-book pr-1"></i>Chapter 3</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/chap3/sensing2/>ロボットシステムにおけるセンシング・アクチュエーション・通信②</a></li><li><a href=/roomba_hack_course/course/chap3/sensing3/>ロボットシステムにおけるセンシング・アクチュエーション・通信③</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap4/><i class="fas fa-book pr-1"></i>Chapter 4</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/chap4/localization/>自己位置推定</a></li><li><a href=/roomba_hack_course/course/chap4/navigation/>ナビゲーション</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap5/><i class="fas fa-book pr-1"></i>Chapter 5</a><ul class="nav docs-sidenav"><li class=active><a href=/roomba_hack_course/course/chap5/three-dimensions/>三次元画像処理</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap6/><i class="fas fa-book pr-1"></i>Chapter 6</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/chap6/service-actionlib/>serviceとactionlib</a></li></ul></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap7/><i class="fas fa-book pr-1"></i>Chapter 7</a></div><div class=docs-toc-item><a class=docs-toc-link href=/roomba_hack_course/course/chap8/><i class="fas fa-book pr-1"></i>Chapter 8</a><ul class="nav docs-sidenav"><li><a href=/roomba_hack_course/course/chap8/final_project/>最終プロジェクト</a></li></ul></div></ul></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>Contents</a></li></ul><nav id=TableOfContents><ul><li><a href=#learn>Learn</a><ul><li><a href=#rgbdカメラについて>RGBDカメラについて</a></li><li><a href=#realsense>RealSense</a></li><li><a href=#物体検出>物体検出</a></li><li><a href=#三次元画像処理>三次元画像処理</a></li><li><a href=#点群の作成>点群の作成</a></li></ul></li><li><a href=#演習>演習</a></li></ul></nav></div><main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role=main><article class=article><div class=docs-article-container><nav class="d-none d-md-flex" aria-label=breadcrumb><ol class=breadcrumb><li class=breadcrumb-item><a href=/roomba_hack_course/>Home</a></li><li class=breadcrumb-item><a href=/roomba_hack_course/course/>ロボットシステム入門</a></li><li class=breadcrumb-item><a href=/roomba_hack_course/course/chap5/>Chapter 5</a></li><li class="breadcrumb-item active" aria-current=page>三次元画像処理</li></ol></nav><h1>三次元画像処理</h1><div class=article-style><h2 id=learn>Learn</h2><p>RGBDカメラを用いて三次元画像処理を行いましょう。</p><h3 id=rgbdカメラについて>RGBDカメラについて</h3><p>RGBDカメラとは、色(RGB)に加え、深度(Depth)情報を取得できるカメラのことです。
周りの環境を三次元の空間として認識することで、ロボットはより複雑にふるまうことができます。
比較的安価でよく利用されるRGBDカメラとして、Intel社製のRealSenseやMicrosoft社製のXtion(エクシオン)などがあります。</p><h3 id=realsense>RealSense</h3><p>今回はRGBDカメラとしてRealSenseD435を使用します。</p><p>ROSで用いる際には<a href=https://github.com/IntelRealSense/realsense-ros target=_blank rel=noopener>標準のラッパー</a>を使用します。</p><pre><code>roslaunch realsense2_camera rs_camera.launch
</code></pre><p>を実行すると、2種類のトピック</p><p><code>/camera/color/image_raw</code> (RGB画像)<br><code>/camera/depth/image_raw</code> (デプス画像)</p><p>が利用できるようになります。<br>これらのトピックはいずれも<code>sensor_msgs/Image</code>型です。</p><p>RealSenseはRGB画像モジュールとデプス画像モジュールが物理的に離れています。
このため、これら2つのトピックはいずれも画像データではあるものの、ピクセルの位置関係が対応しておらず、そのまま画像処理に利用することはできません。</p><p>RealSenseを使用するためのlaunchファイル(<a href>rs_camera.launch</a>)を起動する際に
&ldquo;align_depth"パラメータを"true"に指定することで、デプス画像をRGB画像のピクセルに対応するように変換した
<code>/camera/aligned_depth_to_color/image_raw</code>トピックが使用できるようになります。</p><p>ただし、roomba_bringupパッケージのbringup.launchファイルの</p><pre><code>    &lt;include file=&quot;$(find realsense2_camera)/launch/rs_camera.launch&quot; if=&quot;$(arg realsense)&quot;&gt;
        &lt;arg name=&quot;align_depth&quot; value=&quot;true&quot;/&gt;
    &lt;/include&gt;
</code></pre><p>の箇所がこの操作に対応していため、今回は特別な操作をせずとも
<code>/camera/aligned_depth_to_color/image_raw</code>トピックを使用できます。</p><h3 id=物体検出>物体検出</h3><p>まずは3次元情報を扱わず、RGB画像<code>/camera/color/image_raw</code>のみを用いて画像検出を行ってみましょう。</p><p>以下は、
<a href=https://github.com/matsuolab/roomba_hack/tree/master/catkin_ws/src/three-dimensions_tutorial target=_blank rel=noopener>three-dementions_tutorial</a>パッケージの
<a href=https://github.com/matsuolab/roomba_hack/blob/master/catkin_ws/src/three-dimensions_tutorial/scripts/object_detection.py target=_blank rel=noopener>object_detection.py</a>
です。</p><ol><li><code>/camera/color/image_raw</code>をsubscribeし、</li><li>物体検出アルゴリズムであるYOLOv8に入力し、</li><li>物体検出の結果をbounding boxとして描画し、</li><li><code>/detection_result</code>としてpublish</li></ol><p>の処理を行っています。</p><pre><code class=language-py>#!/usr/bin/env python3

import copy
from typing import List

import cv2
import rospy
from cv_bridge import CvBridge
from sensor_msgs.msg import Image
from ultralytics import YOLO
from ultralytics.engine.results import Results


class ObjectDetection:
    def __init__(self):
        rospy.init_node('object_detection', anonymous=True)

        # Publisher
        self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)

        # Subscriber
        rospy.Subscriber('/camera/color/image_raw', Image, self.callback_rgb)

        self.bridge = CvBridge()
        self.rgb_image = None

        self.model = YOLO('yolov8n.pt')

    def callback_rgb(self, data):
        cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
        self.rgb_image = cv_array

    def process(self):
        while not rospy.is_shutdown():
            if self.rgb_image is None:
                continue

            results: List[Results] = self.model.predict(self.rgb_image)

            # plot bounding box
            tmp_image = copy.deepcopy(self.rgb_image)
            for result in results:
                boxes = result.boxes.cpu().numpy()
                names = result.names
                for xyxy, conf, cls in zip(boxes.xyxy, boxes.conf, boxes.cls):
                    if conf &lt; 0.5:
                        continue
                    x1, y1, x2, y2 = map(int, xyxy[:4])
                    cls_pred = cls
                    tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
                    tmp_image = cv2.putText(tmp_image, names[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)

            # publish image
            detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &quot;bgr8&quot;)
            self.detection_result_pub.publish(detection_result)


if __name__ == '__main__':
    od = ObjectDetection()
    try:
        od.process()
    except rospy.ROSInitException:
        pass
</code></pre><p>画像データを物体検出モデルに入力として渡すためには、その画像データの型がnp.ndarray型である必要があります。
そのため、コールバック関数で、受け取った<code>sensor_msgs/Image</code>型の画像データをnp.ndarray型に変換しています。</p><pre><code class=language-py>cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
</code></pre><p>の部分がこの処理に対応します。<br>subscriberを宣言するときにコールバック関数を指定して、
subscribeしたデータをこの関数に渡すという基本的な処理の流れは、
<code>scan</code>など他のトピックを扱うsubscriberと同じです。</p><p>ここで、YOLOの推論部分をコールバック関数内で行っていないことに注意しましょう。
一見、新しいデータが入ってくるときのみに推論を回すことは合理的に見えますが、
センサの入力に対してコールバック関数内の処理が重いと処理するデータが最新のものからどんどん遅れてしまいます。
コールバック関数はセンサデータの最低限の処理にとどめ、重い処理は分けて書くことを意識しましょう。</p><p>また、ここでは既存の物体検出モジュールを使用しましたが、PyTorchなどで自作したモデルも同様の枠組みで利用することができます。</p><h3 id=三次元画像処理>三次元画像処理</h3><p>次に、RGB画像とデプス画像を統合し、検出した物体までの距離を測定してみましょう。</p><p><code>/camera/color/image_raw</code> (RGB画像)
<code>/camera/aligned_depth_to_color/image_raw</code> (整列されたDepth画像)</p><p>はピクセル同士が対応するように処理されてはいるものの、
パブリッシュされた時刻は独立しているため、
併せて使用するには時刻の同期を行う必要があります。</p><p>画像の時刻同期には<a href=http://wiki.ros.org/message_filters target=_blank rel=noopener>message_filters</a>がよく使われます。</p><p>message_filters.ApproximateTimeSynchronizerを使い、以下のようにSubscriberを作成します。</p><pre><code class=language-python>#!/usr/bin/env python3

import copy
from typing import List

import cv2
import message_filters
import rospy
from cv_bridge import CvBridge
from sensor_msgs.msg import Image

class DetectionDistance:
    def __init__(self):
        rospy.init_node('detection_distance', anonymous=True)

        # Publisher
        self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)

        # Subscriber
        rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image)
        depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
        message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(self.callback_rgbd)

        self.bridge = CvBridge()
        self.rgb_image, self.depth_image = None, None

    def callback_rgbd(self, data1, data2):
        cv_array = self.bridge.imgmsg_to_cv2(data1, 'bgr8')
        cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
        self.rgb_image = cv_array

        cv_array = self.bridge.imgmsg_to_cv2(data2, 'passthrough')
        self.depth_image = cv_array
# 後略
</code></pre><p>この例では、<br><code>/camera/color/image_raw</code>と<br><code>/camera/aligned_depth_to_color/image_raw</code>の<br>トピックを同時(1.0秒までのずれを許容する)に受け取った場合のみ、
2つの画像データをコールバック関数callback_rgbdに渡します。</p><p>それでは、<a href=https://github.com/matsuolab/roomba_hack/tree/master/catkin_ws/src/three-dimensions_tutorial target=_blank rel=noopener>three-dementions_tutorial</a>
パッケージの<a href=https://github.com/matsuolab/roomba_hack/blob/master/catkin_ws/src/three-dimensions_tutorial/scripts/detection_distance.py target=_blank rel=noopener>detection_distance.py</a>
を見てみましょう。<br>物体を検出し、その物体までの距離を測定するスクリプトです。</p><pre><code class=language-py>#!/usr/bin/env python3

import copy
from typing import List

import cv2
import message_filters
import rospy
from cv_bridge import CvBridge
from sensor_msgs.msg import Image
from ultralytics import YOLO
from ultralytics.engine.results import Results


class DetectionDistance:
    def __init__(self):
        rospy.init_node('detection_distance', anonymous=True)

        # Publisher
        self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)

        # Subscriber
        rgb_sub = message_filters.Subscriber('/camera/color/image_raw', Image)
        depth_sub = message_filters.Subscriber('/camera/aligned_depth_to_color/image_raw', Image)
        message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 1.0).registerCallback(self.callback_rgbd)

        self.bridge = CvBridge()
        self.rgb_image, self.depth_image = None, None

        self.model = YOLO('yolov8n.pt')

    def callback_rgbd(self, data1, data2):
        cv_array = self.bridge.imgmsg_to_cv2(data1, 'bgr8')
        cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
        self.rgb_image = cv_array

        cv_array = self.bridge.imgmsg_to_cv2(data2, 'passthrough')
        self.depth_image = cv_array

    def process(self):
        while not rospy.is_shutdown():
            if self.rgb_image is None:
                continue

            # inference
            tmp_image = copy.copy(self.rgb_image)

            results: List[Results] = self.model.predict(self.rgb_image, verbose=False)

            # plot bouding box
            for result in results:
                boxes = result.boxes.cpu().numpy()
                names = result.names
                if len(boxes.xyxy) == 0:
                    continue
                x1, y1, x2, y2 = map(int, boxes.xyxy[0][:4])
                cls_pred = boxes.cls[0]
                tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
                tmp_image = cv2.putText(tmp_image, names[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
                cx, cy = (x1+x2)//2, (y1+y2)//2
                print(names[cls_pred], self.depth_image[cy][cx]/1000, &quot;m&quot;)

            # publish image
            tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR)
            detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &quot;bgr8&quot;)
            self.detection_result_pub.publish(detection_result)


if __name__ == '__main__':
    dd = DetectionDistance()
    try:
        dd.process()
    except rospy.ROSInitException:
        pass
</code></pre><p>基本的には物体検出のスクリプトと同じですが、</p><pre><code class=language-py>cx, cy = (x1+x2)//2, (y1+y2)//2
print(names[cls_pred], self.depth_image[cy][cx]/1000, &quot;m&quot;)
</code></pre><p>でbounding boxの中心座標を変換し、対応する距離をメートル単位で表示しています。</p><p>整列されたデプス画像を用いているため、RGB画像に基づき算出した座標をそのまま指定できます。</p><h3 id=点群の作成>点群の作成</h3><p>上の例ではRGB画像とDepth画像を用いて、物体位置の代表となる点とロボット位置の関係を扱うことができました。</p><p>しかし、代表となる点以外の深度データも三次元空間に直接マッピングできると、
物体の大きさや形状といった情報も扱うことができ、
環境情報をより直感的・統一的に扱うことができるように思われます。</p><p>そこでDepth画像から点群と呼ばれるデータを作成することを考えます。</p><p>点群とは三次元座標値 (X, Y, Z) で構成された点の集まりのことです。各点の情報として、三次元座標値に加え色の情報 (R, G, B) が加わることもあります。
デプス画像はカメラの内部パラメータを用いることによって点群データに変換することができます。(<a href=https://medium.com/yodayoda/from-depth-map-to-point-cloud-7473721d3f target=_blank rel=noopener>参考</a>)</p><p>今回は、デプス画像を点群データに変換するためのROSの外部パッケージである
<a href=http://wiki.ros.org/depth_image_proc target=_blank rel=noopener>depth_image_proc</a>
を使用して点群を作成します。</p><p>外部パッケージは<code>~/catkin_ws/src</code>等のワークスペースに配置し、ビルドしパスを通すことで簡単に使用できます。</p><p>depth_image_procのwikiを参考に以下のようなlaunchファイルを作成しました。</p><pre><code class=language-xml>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;launch&gt;
  &lt;node pkg=&quot;nodelet&quot; type=&quot;nodelet&quot; name=&quot;nodelet_manager&quot; args=&quot;manager&quot; /&gt;

  &lt;node pkg=&quot;nodelet&quot; type=&quot;nodelet&quot; name=&quot;nodelet1&quot;
        args=&quot;load depth_image_proc/point_cloud_xyz nodelet_manager&quot;&gt;
    &lt;remap from=&quot;camera_info&quot; to=&quot;/camera/color/camera_info&quot;/&gt;
    &lt;remap from=&quot;image_rect&quot; to=&quot;/camera/aligned_depth_to_color/image_raw&quot;/&gt;
    &lt;remap from=&quot;points&quot; to=&quot;/camera/depth/points&quot;/&gt;
  &lt;/node&gt;
&lt;/launch&gt;
</code></pre><p>このlaunchファイルを実行すると<br><code>/camera/color/camera_info</code>と<br><code>/camera/aligned_depth_to_color/image_raw</code>を<br>subscribeし、<br><code>/camera/depth/points</code>をpublishするノードが作成されます。</p><p><code>/camera/color/camera_info</code>は
<a href=http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CameraInfo.html target=_blank rel=noopener>sensor_msgs/CameraInfo</a>型のトピックです。
カメラパラメータやフレームid、タイムスタンプなどの情報を保持しており、点群の変換に利用されます。</p><p><code>/camera/aligned_depth_to_color/image_raw</code>はRGB画像に合わせて整列されたDepth画像であるため、
<code>/camera/<mark>depth</mark>/camera_info</code>ではなく<br><code>/camera/<mark>color</mark>/camera_info</code>を指定しています。</p><pre><code class=language-bash>(開発PC)(docker)# roslaunch three-dimensions_tutorial depth2pc.launch
</code></pre><p>を実行し、<code>/camera/depth/points</code>トピックをrvizで可視化をすると三次元空間に点群データが表示されているのが確認できます。</p><h2 id=演習>演習</h2><details class=spoiler id=spoiler-2><summary>(開発PC, jetson)起動準備</summary><p><pre><code class=language-bash>(jetson)$ ./RUN-DOCKER-CONTAINER.sh
(jetson)(docker)# roslaunch roomba_bringup bringup.launch
(開発PC)$ ./RUN-DOCKER-CONTAINER.sh 192.168.10.7x
</code></pre></p></details><details class=spoiler id=spoiler-3><summary>(開発PC)RealSenseのトピックの可視化</summary><p><pre><code class=language-bash>(開発PC)(docker)# rviz
</code></pre><p>rviz上で</p><ul><li><code>/camera/color/image_raw</code></li><li><code>/camera/depth/image_raw</code></li><li><code>/camera/aligned_depth_to_color/image_raw</code></li></ul><p>を可視化して違いを確認してみましょう。</p></p></details><details class=spoiler id=spoiler-4><summary>(開発PC)物体検出を行う</summary><p><pre><code class=language-bash>(開発PC)(docker)# cd catkin_ws; catkin_make; source devel/setup.bash
(開発PC)(docker)# roscd three-dimensions_tutorial; cd yolov3/weights; ./download_weights.sh
(開発PC)(docker)# rosrun three-dimensions_tutorial object_detection.py
# rvizで`/detection_result`を表示し結果を確認してみよう。
(開発PC)(docker)# rosrun three-dimensions_tutorial detection_distance.py
</code></pre></p></details><details class=spoiler id=spoiler-5><summary>(開発PC)外部パッケージを使用</summary><p><pre><code class=language-bash>(開発PC)(docker)# cd ~/external_catkin_ws/src 
(開発PC)(docker)# git clone https://github.com/ros-perception/image_pipeline
(開発PC)(docker)# cd ../; catkin build; source devel/setup.bash
(開発PC)(docker)# cd ~/roomba_hack/catkin_ws; source devel/setup.bash
(開発PC)(docker)# roslaunch three-dimensions_tutorial depth2pc.launch
(開発PC)(docker)# roslaunch navigation_tutorial navigation.launch
</code></pre><p>rvizで<code>/camera/depth/points</code>トピックを追加して確認してみましょう。</p></p></details><details class=spoiler id=spoiler-6><summary>余裕がある人向け</summary><p><p>物体を検出し、特定の物体の手前まで移動するスクリプトを作ってみましょう。</p><p>ヒント</p><ul><li>物体検出結果に基づいて物体部分以外をマスクしたデプス画像をpublishする</li><li>depth2pc.launchでそれをsubscribeし、point(cloud)に変換する</li><li>変換されたpointからmap座標系での位置を取得する</li><li>navigation_tutorial/scripts/set_goal.py (map座標系で指定した位置・姿勢までナビゲーションするスクリプト)などを参考に、その位置へとナビゲーションする</li></ul><p>PyTorchを使用した自作の分類器やネット上の分類器をシステムに組み込んでみましょう。</p><p>Lidarに映らない物体も画像ベースで検出しコストマップに追加することでナビゲーション時にぶつからないようにしましょう。</p></p></details></div><div class=article-widget><div class=post-nav></div></div></div><div class=body-footer><p>Last updated on 2022/1/22</p><p class=edit-page><a href=https://github.com/matsuolab/roomba_hack/edit/master/content/course/chap5/three-dimensions.md><i class="fas fa-pen pr-2"></i>Edit this page</a></p></div></article><footer class=site-footer><p class=powered-by>© 2023 Tokyo Robot And Intelligence Lab (TRAIL)</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></main></div></div></div><div class=page-footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin=anonymous title=mermaid></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/shell.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/bash.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/cpp.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/makefile.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script><script>anchors.add()</script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/roomba_hack_course/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/roomba_hack_course/en/js/wowchemy.min.fd8e182e033e5897f1527e7c94153f3d.js></script></body></html>